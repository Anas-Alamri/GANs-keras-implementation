{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DCGAN on the mnist dataset\n",
    "This code was made after reading both these tutorials, alot of the code is inspired and even copied from them:\n",
    "- https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0\n",
    "- https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the depndencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape, LeakyReLU \n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() #loading the mnist datatset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data needs to be normalized, a simple way of doing that in dividing by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train.max()) #checking the max value in the training data\n",
    "x_train = x_train.astype(\"float32\")/255.  #normalizing the entire training data\n",
    "print(x_train.max())    #then the checking the max after normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought the images are greyscale, we will add a channel dimension of 1 because the convolution layers require them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (60000, 28, 28, 1)\n",
      "train label shape: (60000, 1)\n",
      "test data shape: (10000, 28, 28, 1)\n",
      "test label shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#adding the channel dimension\n",
    "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\n",
    "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],x_test.shape[2],1))\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],-1))\n",
    "y_test = y_test.reshape((y_test.shape[0],-1))\n",
    "\n",
    "#saving the data shapes\n",
    "train_data_shape = x_train.shape\n",
    "test_data_shape = x_test.shape\n",
    "\n",
    "train_label_shape = y_train.shape\n",
    "test_label_shape = y_test.shape\n",
    "\n",
    "#printing the shapes\n",
    "print(\"train data shape:\",train_data_shape)\n",
    "print(\"train label shape:\",train_label_shape)\n",
    "print(\"test data shape:\",test_data_shape)\n",
    "print(\"test label shape:\",test_label_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network models\n",
    "First the Discriminator model is made, its a CNN the has one output that calssifies either tha data is real (1) or is fake (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = Sequential()\n",
    "\n",
    "depth = 64\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "\n",
    "input_shape = train_data_shape[1:]\n",
    "\n",
    "D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "# Out: 1-dim probability\n",
    "\n",
    "D.add(Flatten())\n",
    "\n",
    "D.add(Dense(1))\n",
    "\n",
    "D.add(Activation('sigmoid'))\n",
    "\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "G = Sequential()\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "depth = 64+64+64+64\n",
    "\n",
    "dim = 7\n",
    "\n",
    "# In: 100\n",
    "\n",
    "# Out: dim x dim x depth\n",
    "\n",
    "G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "G.add(Reshape((dim, dim, depth)))\n",
    "\n",
    "G.add(Dropout(dropout))\n",
    "\n",
    "# In: dim x dim x depth\n",
    "\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "\n",
    "G.add(UpSampling2D())\n",
    "\n",
    "G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "G.add(UpSampling2D())\n",
    "\n",
    "G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "\n",
    "G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "\n",
    "G.add(Activation('sigmoid'))\n",
    "\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will deifne the optimizer for the Discriminator model, one of the tutorials above said that RMSprop worked better that Adam, so we used it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer1 = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "D.compile(loss='binary_crossentropy', optimizer=optimizer1,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the adverserial model in which both the generator and discriminator are stacked, we are using a neat trick from keras in which we can stck networks like we are stacking layers, and any update in the adversrial model will affect the generator and discriminator, and any update in the generator or discriminator will affect the adverserial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 28, 28, 1)         2394241   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 6,705,794\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 4,337,089\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "AM = Sequential()\n",
    "AM.add(G)  #stacking the generator\n",
    "AM.add(D)  #stacking the discriminator \n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer2, metrics=['accuracy'])\n",
    "D.trainable = False   #this line isn't important now and you can remove it, it just shows that the discriminator can be untrainable\n",
    "AM.summary()\n",
    "# print(AM.layers[1].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a function used to plot and save the output of the generator, it can take in a noise vector or generate one itself\n",
    "def plot_images(save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % step\n",
    "        images = G.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [train_data_shape[1], train_data_shape[2]])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save2file:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some hyper parameters\n",
    "save_interval = 500   #save a plot that includes the output of the generator every number of steps \n",
    "train_steps=2000      # end number of steps\n",
    "start_step = 0        # start number of steps\n",
    "batch_size=256        # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretraining the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "34000/34000 [==============================] - 30s - loss: 0.1064 - acc: 0.9801 - val_loss: 3.8030e-06 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "images_train = x_train[np.random.randint(0,x_train.shape[0], size=20000), :, :, :]  #selecting random 20k images from training data\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[20000, 100])   #generating random 20k noise vectors\n",
    "images_fake = G.predict(noise)   #generating fake images from the generator\n",
    "x = np.concatenate((images_train, images_fake))  #stacking both real and fake images\n",
    "#generating labels in which real=1, fake =0\n",
    "y = np.ones([2*20000, 1])\n",
    "y[20000:, :] = 0\n",
    "\n",
    "x,y = shuffle(x,y)  #shuffling the data and labels\n",
    "\n",
    "D.trainable = True  #making sure that discriminator is trainable\n",
    "d_loss = D.fit(x, y, batch_size=batch_size, epochs=1, validation_split=0.15) #the pretraining step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535: [D loss: 0.692361, acc: 0.531250]  [A loss: 0.856972, acc: 0.156250]\n",
      "536: [D loss: 0.684546, acc: 0.527344]  [A loss: 0.868491, acc: 0.140625]\n",
      "537: [D loss: 0.691256, acc: 0.535156]  [A loss: 0.906451, acc: 0.117188]\n",
      "538: [D loss: 0.679295, acc: 0.582031]  [A loss: 0.869319, acc: 0.187500]\n",
      "539: [D loss: 0.696165, acc: 0.542969]  [A loss: 0.946788, acc: 0.093750]\n",
      "540: [D loss: 0.764887, acc: 0.492188]  [A loss: 1.074614, acc: 0.031250]\n",
      "541: [D loss: 0.701777, acc: 0.523438]  [A loss: 0.718632, acc: 0.460938]\n",
      "542: [D loss: 0.792859, acc: 0.484375]  [A loss: 1.101982, acc: 0.007812]\n",
      "543: [D loss: 0.699216, acc: 0.503906]  [A loss: 0.770990, acc: 0.273438]\n",
      "544: [D loss: 0.731849, acc: 0.445312]  [A loss: 0.878569, acc: 0.062500]\n",
      "545: [D loss: 0.701624, acc: 0.539062]  [A loss: 0.771065, acc: 0.265625]\n",
      "546: [D loss: 0.705065, acc: 0.507812]  [A loss: 0.799459, acc: 0.218750]\n",
      "547: [D loss: 0.709365, acc: 0.523438]  [A loss: 0.795836, acc: 0.203125]\n",
      "548: [D loss: 0.688226, acc: 0.535156]  [A loss: 0.783720, acc: 0.273438]\n",
      "549: [D loss: 0.696907, acc: 0.515625]  [A loss: 0.827191, acc: 0.195312]\n",
      "550: [D loss: 0.682630, acc: 0.566406]  [A loss: 0.783741, acc: 0.296875]\n",
      "551: [D loss: 0.718542, acc: 0.527344]  [A loss: 0.866970, acc: 0.132812]\n",
      "552: [D loss: 0.691365, acc: 0.519531]  [A loss: 0.838288, acc: 0.101562]\n",
      "553: [D loss: 0.705751, acc: 0.527344]  [A loss: 0.852020, acc: 0.093750]\n",
      "554: [D loss: 0.699991, acc: 0.484375]  [A loss: 0.835493, acc: 0.156250]\n",
      "555: [D loss: 0.694727, acc: 0.558594]  [A loss: 0.901949, acc: 0.085938]\n",
      "556: [D loss: 0.706544, acc: 0.539062]  [A loss: 0.779921, acc: 0.257812]\n",
      "557: [D loss: 0.712611, acc: 0.500000]  [A loss: 0.903812, acc: 0.148438]\n",
      "558: [D loss: 0.697199, acc: 0.531250]  [A loss: 0.886455, acc: 0.093750]\n",
      "559: [D loss: 0.695739, acc: 0.542969]  [A loss: 0.815478, acc: 0.218750]\n",
      "560: [D loss: 0.728905, acc: 0.472656]  [A loss: 0.930174, acc: 0.054688]\n",
      "561: [D loss: 0.690840, acc: 0.531250]  [A loss: 0.753881, acc: 0.328125]\n",
      "562: [D loss: 0.718141, acc: 0.496094]  [A loss: 0.921123, acc: 0.093750]\n",
      "563: [D loss: 0.728753, acc: 0.468750]  [A loss: 0.882806, acc: 0.093750]\n",
      "564: [D loss: 0.776396, acc: 0.488281]  [A loss: 1.074007, acc: 0.007812]\n",
      "565: [D loss: 0.704425, acc: 0.492188]  [A loss: 0.778832, acc: 0.312500]\n",
      "566: [D loss: 0.714305, acc: 0.511719]  [A loss: 0.851040, acc: 0.109375]\n",
      "567: [D loss: 0.703769, acc: 0.511719]  [A loss: 0.777522, acc: 0.289062]\n",
      "568: [D loss: 0.699905, acc: 0.523438]  [A loss: 0.792132, acc: 0.195312]\n",
      "569: [D loss: 0.692059, acc: 0.515625]  [A loss: 0.750617, acc: 0.359375]\n",
      "570: [D loss: 0.700476, acc: 0.531250]  [A loss: 0.862803, acc: 0.101562]\n",
      "571: [D loss: 0.699464, acc: 0.523438]  [A loss: 0.829834, acc: 0.179688]\n",
      "572: [D loss: 0.707318, acc: 0.527344]  [A loss: 0.959386, acc: 0.101562]\n",
      "573: [D loss: 0.686337, acc: 0.542969]  [A loss: 0.808568, acc: 0.187500]\n",
      "574: [D loss: 0.706038, acc: 0.519531]  [A loss: 0.893414, acc: 0.101562]\n",
      "575: [D loss: 0.692711, acc: 0.546875]  [A loss: 0.820652, acc: 0.218750]\n",
      "576: [D loss: 0.690117, acc: 0.566406]  [A loss: 0.785563, acc: 0.296875]\n",
      "577: [D loss: 0.715663, acc: 0.515625]  [A loss: 0.839158, acc: 0.140625]\n",
      "578: [D loss: 0.706772, acc: 0.535156]  [A loss: 0.856543, acc: 0.140625]\n",
      "579: [D loss: 0.690174, acc: 0.554688]  [A loss: 0.828328, acc: 0.218750]\n",
      "580: [D loss: 0.700531, acc: 0.523438]  [A loss: 0.874176, acc: 0.140625]\n",
      "581: [D loss: 0.684518, acc: 0.546875]  [A loss: 0.768598, acc: 0.375000]\n",
      "582: [D loss: 0.723648, acc: 0.507812]  [A loss: 0.874344, acc: 0.101562]\n",
      "583: [D loss: 0.703617, acc: 0.527344]  [A loss: 0.816011, acc: 0.210938]\n",
      "584: [D loss: 0.703562, acc: 0.515625]  [A loss: 0.874876, acc: 0.078125]\n",
      "585: [D loss: 0.677602, acc: 0.574219]  [A loss: 0.807052, acc: 0.218750]\n",
      "586: [D loss: 0.701294, acc: 0.519531]  [A loss: 0.913999, acc: 0.101562]\n",
      "587: [D loss: 0.703533, acc: 0.511719]  [A loss: 0.788039, acc: 0.242188]\n",
      "588: [D loss: 0.746715, acc: 0.476562]  [A loss: 1.014604, acc: 0.070312]\n",
      "589: [D loss: 0.723801, acc: 0.492188]  [A loss: 0.915177, acc: 0.109375]\n",
      "590: [D loss: 0.707666, acc: 0.488281]  [A loss: 0.767096, acc: 0.359375]\n",
      "591: [D loss: 0.698681, acc: 0.546875]  [A loss: 0.874814, acc: 0.125000]\n",
      "592: [D loss: 0.700939, acc: 0.539062]  [A loss: 0.928385, acc: 0.101562]\n",
      "593: [D loss: 0.680597, acc: 0.574219]  [A loss: 0.769820, acc: 0.289062]\n",
      "594: [D loss: 0.707104, acc: 0.515625]  [A loss: 0.874791, acc: 0.125000]\n",
      "595: [D loss: 0.693030, acc: 0.535156]  [A loss: 0.817076, acc: 0.195312]\n",
      "596: [D loss: 0.717212, acc: 0.496094]  [A loss: 0.806609, acc: 0.171875]\n",
      "597: [D loss: 0.709033, acc: 0.535156]  [A loss: 0.896732, acc: 0.101562]\n",
      "598: [D loss: 0.692516, acc: 0.527344]  [A loss: 0.783692, acc: 0.218750]\n",
      "599: [D loss: 0.694747, acc: 0.535156]  [A loss: 0.855269, acc: 0.140625]\n",
      "600: [D loss: 0.689490, acc: 0.558594]  [A loss: 0.863299, acc: 0.171875]\n",
      "601: [D loss: 0.735874, acc: 0.457031]  [A loss: 0.878866, acc: 0.125000]\n",
      "602: [D loss: 0.721529, acc: 0.535156]  [A loss: 1.068903, acc: 0.023438]\n",
      "603: [D loss: 0.680400, acc: 0.558594]  [A loss: 0.703661, acc: 0.437500]\n",
      "604: [D loss: 0.748126, acc: 0.496094]  [A loss: 0.958719, acc: 0.046875]\n",
      "605: [D loss: 0.687174, acc: 0.535156]  [A loss: 0.805982, acc: 0.226562]\n",
      "606: [D loss: 0.729706, acc: 0.500000]  [A loss: 0.957395, acc: 0.039062]\n",
      "607: [D loss: 0.688224, acc: 0.519531]  [A loss: 0.721116, acc: 0.414062]\n",
      "608: [D loss: 0.726508, acc: 0.507812]  [A loss: 1.024414, acc: 0.046875]\n",
      "609: [D loss: 0.673497, acc: 0.593750]  [A loss: 0.736793, acc: 0.390625]\n",
      "610: [D loss: 0.725461, acc: 0.480469]  [A loss: 0.861124, acc: 0.125000]\n",
      "611: [D loss: 0.691215, acc: 0.492188]  [A loss: 0.789760, acc: 0.351562]\n",
      "612: [D loss: 0.690028, acc: 0.523438]  [A loss: 0.804771, acc: 0.265625]\n",
      "613: [D loss: 0.732233, acc: 0.472656]  [A loss: 0.853676, acc: 0.179688]\n",
      "614: [D loss: 0.696727, acc: 0.496094]  [A loss: 0.890792, acc: 0.132812]\n",
      "615: [D loss: 0.699828, acc: 0.562500]  [A loss: 0.814403, acc: 0.226562]\n",
      "616: [D loss: 0.687509, acc: 0.546875]  [A loss: 0.844349, acc: 0.164062]\n",
      "617: [D loss: 0.697309, acc: 0.515625]  [A loss: 0.796399, acc: 0.250000]\n",
      "618: [D loss: 0.697379, acc: 0.558594]  [A loss: 0.853293, acc: 0.164062]\n",
      "619: [D loss: 0.701253, acc: 0.515625]  [A loss: 0.844213, acc: 0.171875]\n",
      "620: [D loss: 0.724784, acc: 0.464844]  [A loss: 0.898612, acc: 0.125000]\n",
      "621: [D loss: 0.698880, acc: 0.519531]  [A loss: 0.798195, acc: 0.296875]\n",
      "622: [D loss: 0.696905, acc: 0.542969]  [A loss: 0.842978, acc: 0.250000]\n",
      "623: [D loss: 0.703943, acc: 0.531250]  [A loss: 0.849407, acc: 0.171875]\n",
      "624: [D loss: 0.702699, acc: 0.535156]  [A loss: 0.907192, acc: 0.109375]\n",
      "625: [D loss: 0.701215, acc: 0.519531]  [A loss: 0.866147, acc: 0.156250]\n",
      "626: [D loss: 0.738006, acc: 0.484375]  [A loss: 1.009673, acc: 0.015625]\n",
      "627: [D loss: 0.701780, acc: 0.539062]  [A loss: 0.803258, acc: 0.234375]\n",
      "628: [D loss: 0.715525, acc: 0.488281]  [A loss: 0.886748, acc: 0.062500]\n",
      "629: [D loss: 0.679808, acc: 0.484375]  [A loss: 0.809921, acc: 0.242188]\n",
      "630: [D loss: 0.703057, acc: 0.523438]  [A loss: 0.852673, acc: 0.195312]\n",
      "631: [D loss: 0.725454, acc: 0.500000]  [A loss: 0.913483, acc: 0.148438]\n",
      "632: [D loss: 0.686112, acc: 0.589844]  [A loss: 0.756449, acc: 0.312500]\n",
      "633: [D loss: 0.723342, acc: 0.488281]  [A loss: 1.030480, acc: 0.062500]\n",
      "634: [D loss: 0.714855, acc: 0.500000]  [A loss: 0.989152, acc: 0.085938]\n",
      "635: [D loss: 0.690988, acc: 0.492188]  [A loss: 0.836363, acc: 0.210938]\n",
      "636: [D loss: 0.728358, acc: 0.519531]  [A loss: 0.973128, acc: 0.031250]\n",
      "637: [D loss: 0.681947, acc: 0.554688]  [A loss: 0.759860, acc: 0.335938]\n",
      "638: [D loss: 0.776273, acc: 0.492188]  [A loss: 0.884594, acc: 0.070312]\n",
      "639: [D loss: 0.699530, acc: 0.539062]  [A loss: 0.805349, acc: 0.210938]\n",
      "640: [D loss: 0.724926, acc: 0.480469]  [A loss: 0.932799, acc: 0.085938]\n",
      "641: [D loss: 0.694820, acc: 0.546875]  [A loss: 0.763763, acc: 0.367188]\n",
      "642: [D loss: 0.715977, acc: 0.535156]  [A loss: 0.871593, acc: 0.093750]\n",
      "643: [D loss: 0.693390, acc: 0.539062]  [A loss: 0.843402, acc: 0.210938]\n",
      "644: [D loss: 0.687850, acc: 0.539062]  [A loss: 0.904045, acc: 0.093750]\n",
      "645: [D loss: 0.712073, acc: 0.515625]  [A loss: 0.808193, acc: 0.226562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646: [D loss: 0.721429, acc: 0.480469]  [A loss: 0.848438, acc: 0.132812]\n",
      "647: [D loss: 0.693031, acc: 0.539062]  [A loss: 0.820052, acc: 0.273438]\n",
      "648: [D loss: 0.684155, acc: 0.554688]  [A loss: 0.854570, acc: 0.195312]\n",
      "649: [D loss: 0.693051, acc: 0.554688]  [A loss: 0.844370, acc: 0.234375]\n",
      "650: [D loss: 0.719260, acc: 0.511719]  [A loss: 0.827939, acc: 0.265625]\n",
      "651: [D loss: 0.712193, acc: 0.519531]  [A loss: 0.917550, acc: 0.109375]\n",
      "652: [D loss: 0.694300, acc: 0.515625]  [A loss: 0.808841, acc: 0.242188]\n",
      "653: [D loss: 0.714602, acc: 0.492188]  [A loss: 0.883237, acc: 0.179688]\n",
      "654: [D loss: 0.691032, acc: 0.566406]  [A loss: 0.768171, acc: 0.281250]\n",
      "655: [D loss: 0.706196, acc: 0.507812]  [A loss: 0.979364, acc: 0.078125]\n",
      "656: [D loss: 0.718573, acc: 0.496094]  [A loss: 0.888513, acc: 0.085938]\n",
      "657: [D loss: 0.691185, acc: 0.558594]  [A loss: 0.832599, acc: 0.203125]\n",
      "658: [D loss: 0.696828, acc: 0.523438]  [A loss: 0.843319, acc: 0.179688]\n",
      "659: [D loss: 0.701121, acc: 0.492188]  [A loss: 0.949501, acc: 0.078125]\n",
      "660: [D loss: 0.713928, acc: 0.441406]  [A loss: 0.835646, acc: 0.171875]\n",
      "661: [D loss: 0.715929, acc: 0.472656]  [A loss: 0.932690, acc: 0.085938]\n",
      "662: [D loss: 0.693445, acc: 0.558594]  [A loss: 0.878092, acc: 0.117188]\n",
      "663: [D loss: 0.700047, acc: 0.523438]  [A loss: 1.000595, acc: 0.039062]\n",
      "664: [D loss: 0.691279, acc: 0.531250]  [A loss: 0.829398, acc: 0.210938]\n",
      "665: [D loss: 0.711820, acc: 0.503906]  [A loss: 0.922830, acc: 0.085938]\n",
      "666: [D loss: 0.691332, acc: 0.531250]  [A loss: 0.815450, acc: 0.328125]\n",
      "667: [D loss: 0.694215, acc: 0.578125]  [A loss: 0.952200, acc: 0.148438]\n",
      "668: [D loss: 0.732584, acc: 0.449219]  [A loss: 0.903836, acc: 0.070312]\n",
      "669: [D loss: 0.709868, acc: 0.519531]  [A loss: 0.927921, acc: 0.101562]\n",
      "670: [D loss: 0.686874, acc: 0.519531]  [A loss: 0.819385, acc: 0.257812]\n",
      "671: [D loss: 0.699186, acc: 0.539062]  [A loss: 0.974650, acc: 0.132812]\n",
      "672: [D loss: 0.689942, acc: 0.531250]  [A loss: 0.802719, acc: 0.312500]\n",
      "673: [D loss: 0.722905, acc: 0.503906]  [A loss: 0.942737, acc: 0.093750]\n",
      "674: [D loss: 0.714128, acc: 0.484375]  [A loss: 0.771718, acc: 0.320312]\n",
      "675: [D loss: 0.720487, acc: 0.515625]  [A loss: 1.008332, acc: 0.046875]\n",
      "676: [D loss: 0.698524, acc: 0.535156]  [A loss: 0.742268, acc: 0.382812]\n",
      "677: [D loss: 0.721262, acc: 0.511719]  [A loss: 0.943611, acc: 0.078125]\n",
      "678: [D loss: 0.736018, acc: 0.410156]  [A loss: 0.817647, acc: 0.187500]\n",
      "679: [D loss: 0.697277, acc: 0.527344]  [A loss: 0.898800, acc: 0.171875]\n",
      "680: [D loss: 0.710839, acc: 0.515625]  [A loss: 0.804268, acc: 0.289062]\n",
      "681: [D loss: 0.705893, acc: 0.539062]  [A loss: 0.906434, acc: 0.132812]\n",
      "682: [D loss: 0.682242, acc: 0.570312]  [A loss: 0.798721, acc: 0.273438]\n",
      "683: [D loss: 0.717242, acc: 0.523438]  [A loss: 0.942939, acc: 0.078125]\n",
      "684: [D loss: 0.688844, acc: 0.546875]  [A loss: 0.806894, acc: 0.265625]\n",
      "685: [D loss: 0.725113, acc: 0.468750]  [A loss: 0.916822, acc: 0.093750]\n",
      "686: [D loss: 0.695580, acc: 0.511719]  [A loss: 0.826804, acc: 0.195312]\n",
      "687: [D loss: 0.695090, acc: 0.562500]  [A loss: 0.868637, acc: 0.164062]\n",
      "688: [D loss: 0.686314, acc: 0.546875]  [A loss: 0.861170, acc: 0.179688]\n",
      "689: [D loss: 0.678616, acc: 0.570312]  [A loss: 0.920793, acc: 0.125000]\n",
      "690: [D loss: 0.701867, acc: 0.503906]  [A loss: 0.889392, acc: 0.132812]\n",
      "691: [D loss: 0.717860, acc: 0.480469]  [A loss: 0.997095, acc: 0.023438]\n",
      "692: [D loss: 0.705153, acc: 0.507812]  [A loss: 0.778973, acc: 0.359375]\n",
      "693: [D loss: 0.703173, acc: 0.535156]  [A loss: 0.923318, acc: 0.148438]\n",
      "694: [D loss: 0.708272, acc: 0.488281]  [A loss: 0.879855, acc: 0.195312]\n",
      "695: [D loss: 0.738698, acc: 0.500000]  [A loss: 1.049412, acc: 0.023438]\n",
      "696: [D loss: 0.706846, acc: 0.511719]  [A loss: 0.782043, acc: 0.281250]\n",
      "697: [D loss: 0.700929, acc: 0.535156]  [A loss: 0.918577, acc: 0.093750]\n",
      "698: [D loss: 0.694484, acc: 0.578125]  [A loss: 0.928913, acc: 0.117188]\n",
      "699: [D loss: 0.709113, acc: 0.523438]  [A loss: 0.912956, acc: 0.101562]\n",
      "700: [D loss: 0.684221, acc: 0.574219]  [A loss: 0.915202, acc: 0.125000]\n",
      "701: [D loss: 0.710386, acc: 0.511719]  [A loss: 0.958401, acc: 0.101562]\n",
      "702: [D loss: 0.696042, acc: 0.511719]  [A loss: 0.966791, acc: 0.078125]\n",
      "703: [D loss: 0.676226, acc: 0.597656]  [A loss: 0.796662, acc: 0.367188]\n",
      "704: [D loss: 0.712044, acc: 0.515625]  [A loss: 0.865394, acc: 0.164062]\n",
      "705: [D loss: 0.729415, acc: 0.492188]  [A loss: 1.068360, acc: 0.007812]\n",
      "706: [D loss: 0.686372, acc: 0.546875]  [A loss: 0.786632, acc: 0.328125]\n",
      "707: [D loss: 0.728251, acc: 0.542969]  [A loss: 0.992980, acc: 0.015625]\n",
      "708: [D loss: 0.710347, acc: 0.500000]  [A loss: 0.800750, acc: 0.203125]\n",
      "709: [D loss: 0.714390, acc: 0.503906]  [A loss: 0.898844, acc: 0.164062]\n",
      "710: [D loss: 0.699393, acc: 0.531250]  [A loss: 0.773812, acc: 0.320312]\n",
      "711: [D loss: 0.720207, acc: 0.511719]  [A loss: 0.926302, acc: 0.078125]\n",
      "712: [D loss: 0.701694, acc: 0.500000]  [A loss: 0.864159, acc: 0.203125]\n",
      "713: [D loss: 0.694547, acc: 0.531250]  [A loss: 0.838370, acc: 0.234375]\n",
      "714: [D loss: 0.713642, acc: 0.503906]  [A loss: 1.192235, acc: 0.046875]\n",
      "715: [D loss: 0.679982, acc: 0.554688]  [A loss: 0.693623, acc: 0.523438]\n",
      "716: [D loss: 0.746400, acc: 0.523438]  [A loss: 1.133462, acc: 0.015625]\n",
      "717: [D loss: 0.720680, acc: 0.500000]  [A loss: 0.912901, acc: 0.171875]\n",
      "718: [D loss: 0.665634, acc: 0.628906]  [A loss: 0.810035, acc: 0.281250]\n",
      "719: [D loss: 0.703483, acc: 0.527344]  [A loss: 0.870794, acc: 0.164062]\n",
      "720: [D loss: 0.721683, acc: 0.523438]  [A loss: 0.954884, acc: 0.109375]\n",
      "721: [D loss: 0.704308, acc: 0.500000]  [A loss: 0.819283, acc: 0.234375]\n",
      "722: [D loss: 0.704123, acc: 0.515625]  [A loss: 0.852666, acc: 0.210938]\n",
      "723: [D loss: 0.706437, acc: 0.523438]  [A loss: 0.808286, acc: 0.265625]\n",
      "724: [D loss: 0.699232, acc: 0.550781]  [A loss: 0.840880, acc: 0.187500]\n",
      "725: [D loss: 0.701177, acc: 0.503906]  [A loss: 0.810371, acc: 0.281250]\n",
      "726: [D loss: 0.714478, acc: 0.511719]  [A loss: 0.884223, acc: 0.101562]\n",
      "727: [D loss: 0.761711, acc: 0.437500]  [A loss: 1.045199, acc: 0.054688]\n",
      "728: [D loss: 0.733621, acc: 0.472656]  [A loss: 0.928818, acc: 0.085938]\n",
      "729: [D loss: 0.692184, acc: 0.535156]  [A loss: 0.853370, acc: 0.156250]\n",
      "730: [D loss: 0.700585, acc: 0.488281]  [A loss: 0.864197, acc: 0.210938]\n",
      "731: [D loss: 0.722615, acc: 0.464844]  [A loss: 0.805500, acc: 0.296875]\n",
      "732: [D loss: 0.704483, acc: 0.488281]  [A loss: 0.921667, acc: 0.117188]\n",
      "733: [D loss: 0.692895, acc: 0.523438]  [A loss: 0.747575, acc: 0.359375]\n",
      "734: [D loss: 0.713227, acc: 0.507812]  [A loss: 0.851239, acc: 0.218750]\n",
      "735: [D loss: 0.706069, acc: 0.507812]  [A loss: 0.752886, acc: 0.398438]\n",
      "736: [D loss: 0.749168, acc: 0.496094]  [A loss: 1.077518, acc: 0.015625]\n",
      "737: [D loss: 0.699183, acc: 0.527344]  [A loss: 0.780048, acc: 0.382812]\n",
      "738: [D loss: 0.757979, acc: 0.507812]  [A loss: 0.979225, acc: 0.062500]\n",
      "739: [D loss: 0.704243, acc: 0.484375]  [A loss: 0.778977, acc: 0.343750]\n",
      "740: [D loss: 0.725047, acc: 0.503906]  [A loss: 0.941607, acc: 0.101562]\n",
      "741: [D loss: 0.699815, acc: 0.503906]  [A loss: 0.736894, acc: 0.414062]\n",
      "742: [D loss: 0.702948, acc: 0.542969]  [A loss: 0.826130, acc: 0.234375]\n",
      "743: [D loss: 0.696909, acc: 0.546875]  [A loss: 0.824841, acc: 0.203125]\n",
      "744: [D loss: 0.708564, acc: 0.523438]  [A loss: 0.841719, acc: 0.218750]\n",
      "745: [D loss: 0.701332, acc: 0.562500]  [A loss: 0.807208, acc: 0.218750]\n",
      "746: [D loss: 0.697834, acc: 0.542969]  [A loss: 0.886183, acc: 0.171875]\n",
      "747: [D loss: 0.710446, acc: 0.515625]  [A loss: 0.880950, acc: 0.195312]\n",
      "748: [D loss: 0.705736, acc: 0.519531]  [A loss: 0.930920, acc: 0.109375]\n",
      "749: [D loss: 0.734152, acc: 0.476562]  [A loss: 1.268663, acc: 0.015625]\n",
      "750: [D loss: 0.669980, acc: 0.621094]  [A loss: 0.672474, acc: 0.531250]\n",
      "751: [D loss: 0.778693, acc: 0.515625]  [A loss: 1.123537, acc: 0.000000]\n",
      "752: [D loss: 0.721573, acc: 0.457031]  [A loss: 0.791402, acc: 0.265625]\n",
      "753: [D loss: 0.712329, acc: 0.496094]  [A loss: 0.875214, acc: 0.132812]\n",
      "754: [D loss: 0.689445, acc: 0.562500]  [A loss: 0.835239, acc: 0.265625]\n",
      "755: [D loss: 0.694844, acc: 0.554688]  [A loss: 0.848126, acc: 0.234375]\n",
      "756: [D loss: 0.700346, acc: 0.527344]  [A loss: 0.788997, acc: 0.304688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757: [D loss: 0.696717, acc: 0.511719]  [A loss: 0.871542, acc: 0.218750]\n",
      "758: [D loss: 0.694193, acc: 0.519531]  [A loss: 0.839393, acc: 0.195312]\n",
      "759: [D loss: 0.712574, acc: 0.500000]  [A loss: 0.847031, acc: 0.171875]\n",
      "760: [D loss: 0.706660, acc: 0.480469]  [A loss: 0.827273, acc: 0.203125]\n",
      "761: [D loss: 0.707331, acc: 0.519531]  [A loss: 0.935710, acc: 0.125000]\n",
      "762: [D loss: 0.712250, acc: 0.496094]  [A loss: 0.800128, acc: 0.367188]\n",
      "763: [D loss: 0.707750, acc: 0.535156]  [A loss: 0.935853, acc: 0.101562]\n",
      "764: [D loss: 0.683483, acc: 0.542969]  [A loss: 0.842669, acc: 0.218750]\n",
      "765: [D loss: 0.703163, acc: 0.519531]  [A loss: 0.893234, acc: 0.140625]\n",
      "766: [D loss: 0.680632, acc: 0.582031]  [A loss: 0.809721, acc: 0.273438]\n",
      "767: [D loss: 0.688045, acc: 0.546875]  [A loss: 0.924874, acc: 0.132812]\n",
      "768: [D loss: 0.705710, acc: 0.523438]  [A loss: 0.845727, acc: 0.171875]\n",
      "769: [D loss: 0.680725, acc: 0.566406]  [A loss: 0.842746, acc: 0.234375]\n",
      "770: [D loss: 0.705359, acc: 0.554688]  [A loss: 0.897106, acc: 0.093750]\n",
      "771: [D loss: 0.698197, acc: 0.546875]  [A loss: 0.769479, acc: 0.335938]\n",
      "772: [D loss: 0.693584, acc: 0.535156]  [A loss: 0.947492, acc: 0.109375]\n",
      "773: [D loss: 0.702730, acc: 0.496094]  [A loss: 0.832859, acc: 0.242188]\n",
      "774: [D loss: 0.726091, acc: 0.503906]  [A loss: 0.932186, acc: 0.148438]\n",
      "775: [D loss: 0.713573, acc: 0.527344]  [A loss: 0.908439, acc: 0.085938]\n",
      "776: [D loss: 0.733513, acc: 0.457031]  [A loss: 0.961320, acc: 0.070312]\n",
      "777: [D loss: 0.698556, acc: 0.511719]  [A loss: 0.804595, acc: 0.273438]\n",
      "778: [D loss: 0.722929, acc: 0.484375]  [A loss: 0.982907, acc: 0.093750]\n",
      "779: [D loss: 0.685983, acc: 0.562500]  [A loss: 0.796734, acc: 0.328125]\n",
      "780: [D loss: 0.715576, acc: 0.511719]  [A loss: 0.918243, acc: 0.156250]\n",
      "781: [D loss: 0.695894, acc: 0.535156]  [A loss: 0.966317, acc: 0.156250]\n",
      "782: [D loss: 0.692403, acc: 0.539062]  [A loss: 1.077176, acc: 0.070312]\n",
      "783: [D loss: 0.704960, acc: 0.550781]  [A loss: 0.995061, acc: 0.070312]\n",
      "784: [D loss: 0.696638, acc: 0.511719]  [A loss: 0.830165, acc: 0.234375]\n",
      "785: [D loss: 0.727177, acc: 0.496094]  [A loss: 0.991058, acc: 0.062500]\n",
      "786: [D loss: 0.690945, acc: 0.574219]  [A loss: 0.817850, acc: 0.273438]\n",
      "787: [D loss: 0.708365, acc: 0.480469]  [A loss: 0.869463, acc: 0.210938]\n",
      "788: [D loss: 0.717099, acc: 0.503906]  [A loss: 0.932108, acc: 0.109375]\n",
      "789: [D loss: 0.710383, acc: 0.523438]  [A loss: 1.035246, acc: 0.093750]\n",
      "790: [D loss: 0.683208, acc: 0.515625]  [A loss: 0.856674, acc: 0.203125]\n",
      "791: [D loss: 0.775781, acc: 0.488281]  [A loss: 1.136789, acc: 0.039062]\n",
      "792: [D loss: 0.677398, acc: 0.558594]  [A loss: 0.671802, acc: 0.484375]\n",
      "793: [D loss: 0.919014, acc: 0.488281]  [A loss: 1.212352, acc: 0.000000]\n",
      "794: [D loss: 0.691981, acc: 0.582031]  [A loss: 0.777128, acc: 0.320312]\n",
      "795: [D loss: 0.810196, acc: 0.531250]  [A loss: 1.146427, acc: 0.000000]\n",
      "796: [D loss: 0.701327, acc: 0.511719]  [A loss: 0.718810, acc: 0.468750]\n",
      "797: [D loss: 0.771397, acc: 0.488281]  [A loss: 1.038354, acc: 0.023438]\n",
      "798: [D loss: 0.726707, acc: 0.488281]  [A loss: 0.827925, acc: 0.179688]\n",
      "799: [D loss: 0.688225, acc: 0.535156]  [A loss: 0.843036, acc: 0.164062]\n",
      "800: [D loss: 0.715522, acc: 0.496094]  [A loss: 0.732548, acc: 0.421875]\n",
      "801: [D loss: 0.709510, acc: 0.507812]  [A loss: 0.814049, acc: 0.265625]\n",
      "802: [D loss: 0.721513, acc: 0.542969]  [A loss: 0.868276, acc: 0.218750]\n",
      "803: [D loss: 0.704127, acc: 0.574219]  [A loss: 0.797002, acc: 0.328125]\n",
      "804: [D loss: 0.699039, acc: 0.519531]  [A loss: 0.842276, acc: 0.195312]\n",
      "805: [D loss: 0.693125, acc: 0.519531]  [A loss: 0.788781, acc: 0.281250]\n",
      "806: [D loss: 0.725314, acc: 0.507812]  [A loss: 0.928732, acc: 0.109375]\n",
      "807: [D loss: 0.709763, acc: 0.492188]  [A loss: 0.902430, acc: 0.125000]\n",
      "808: [D loss: 0.699547, acc: 0.503906]  [A loss: 0.809866, acc: 0.304688]\n",
      "809: [D loss: 0.700020, acc: 0.523438]  [A loss: 0.837186, acc: 0.195312]\n",
      "810: [D loss: 0.691487, acc: 0.550781]  [A loss: 0.867437, acc: 0.171875]\n",
      "811: [D loss: 0.708888, acc: 0.527344]  [A loss: 0.835834, acc: 0.210938]\n",
      "812: [D loss: 0.697537, acc: 0.554688]  [A loss: 0.876277, acc: 0.187500]\n",
      "813: [D loss: 0.686059, acc: 0.554688]  [A loss: 0.906860, acc: 0.132812]\n",
      "814: [D loss: 0.693145, acc: 0.511719]  [A loss: 0.789531, acc: 0.343750]\n",
      "815: [D loss: 0.709943, acc: 0.511719]  [A loss: 0.933606, acc: 0.164062]\n",
      "816: [D loss: 0.695610, acc: 0.550781]  [A loss: 0.850149, acc: 0.210938]\n",
      "817: [D loss: 0.712069, acc: 0.562500]  [A loss: 0.877288, acc: 0.226562]\n",
      "818: [D loss: 0.704980, acc: 0.546875]  [A loss: 0.963158, acc: 0.070312]\n",
      "819: [D loss: 0.708246, acc: 0.480469]  [A loss: 0.949656, acc: 0.125000]\n",
      "820: [D loss: 0.700847, acc: 0.542969]  [A loss: 0.828375, acc: 0.218750]\n",
      "821: [D loss: 0.710041, acc: 0.527344]  [A loss: 0.886223, acc: 0.132812]\n",
      "822: [D loss: 0.707204, acc: 0.500000]  [A loss: 0.851377, acc: 0.203125]\n",
      "823: [D loss: 0.723743, acc: 0.480469]  [A loss: 0.971510, acc: 0.078125]\n",
      "824: [D loss: 0.727810, acc: 0.453125]  [A loss: 0.898466, acc: 0.210938]\n",
      "825: [D loss: 0.711095, acc: 0.535156]  [A loss: 1.045174, acc: 0.039062]\n",
      "826: [D loss: 0.696470, acc: 0.546875]  [A loss: 0.826803, acc: 0.250000]\n",
      "827: [D loss: 0.709008, acc: 0.519531]  [A loss: 0.859966, acc: 0.304688]\n",
      "828: [D loss: 0.721858, acc: 0.496094]  [A loss: 0.974519, acc: 0.062500]\n",
      "829: [D loss: 0.697567, acc: 0.527344]  [A loss: 0.869836, acc: 0.203125]\n",
      "830: [D loss: 0.718382, acc: 0.496094]  [A loss: 0.990524, acc: 0.062500]\n",
      "831: [D loss: 0.716208, acc: 0.496094]  [A loss: 0.836132, acc: 0.242188]\n",
      "832: [D loss: 0.683322, acc: 0.562500]  [A loss: 0.871789, acc: 0.289062]\n",
      "833: [D loss: 0.702939, acc: 0.578125]  [A loss: 1.087189, acc: 0.054688]\n",
      "834: [D loss: 0.706302, acc: 0.496094]  [A loss: 0.789459, acc: 0.382812]\n",
      "835: [D loss: 0.746616, acc: 0.488281]  [A loss: 1.077674, acc: 0.039062]\n",
      "836: [D loss: 0.741060, acc: 0.503906]  [A loss: 0.993278, acc: 0.109375]\n",
      "837: [D loss: 0.718021, acc: 0.496094]  [A loss: 0.979369, acc: 0.062500]\n",
      "838: [D loss: 0.722009, acc: 0.480469]  [A loss: 0.845690, acc: 0.226562]\n",
      "839: [D loss: 0.716811, acc: 0.511719]  [A loss: 0.976061, acc: 0.109375]\n",
      "840: [D loss: 0.732682, acc: 0.492188]  [A loss: 0.870275, acc: 0.179688]\n",
      "841: [D loss: 0.681939, acc: 0.554688]  [A loss: 0.883950, acc: 0.257812]\n",
      "842: [D loss: 0.711236, acc: 0.511719]  [A loss: 0.959891, acc: 0.085938]\n",
      "843: [D loss: 0.693591, acc: 0.523438]  [A loss: 0.819673, acc: 0.273438]\n",
      "844: [D loss: 0.705332, acc: 0.535156]  [A loss: 1.057503, acc: 0.062500]\n",
      "845: [D loss: 0.708422, acc: 0.503906]  [A loss: 0.956067, acc: 0.117188]\n",
      "846: [D loss: 0.712399, acc: 0.515625]  [A loss: 0.918769, acc: 0.117188]\n",
      "847: [D loss: 0.712448, acc: 0.531250]  [A loss: 0.940669, acc: 0.085938]\n",
      "848: [D loss: 0.709448, acc: 0.480469]  [A loss: 0.901075, acc: 0.195312]\n",
      "849: [D loss: 0.663428, acc: 0.601562]  [A loss: 0.915722, acc: 0.156250]\n",
      "850: [D loss: 0.684062, acc: 0.585938]  [A loss: 1.013167, acc: 0.101562]\n",
      "851: [D loss: 0.688828, acc: 0.578125]  [A loss: 0.868532, acc: 0.195312]\n",
      "852: [D loss: 0.711034, acc: 0.558594]  [A loss: 1.102293, acc: 0.085938]\n",
      "853: [D loss: 0.741326, acc: 0.488281]  [A loss: 0.934125, acc: 0.156250]\n",
      "854: [D loss: 0.707918, acc: 0.511719]  [A loss: 0.836136, acc: 0.218750]\n",
      "855: [D loss: 0.715096, acc: 0.542969]  [A loss: 1.095976, acc: 0.039062]\n",
      "856: [D loss: 0.684986, acc: 0.550781]  [A loss: 0.779300, acc: 0.390625]\n",
      "857: [D loss: 0.750091, acc: 0.515625]  [A loss: 1.169229, acc: 0.031250]\n",
      "858: [D loss: 0.745464, acc: 0.429688]  [A loss: 0.778594, acc: 0.351562]\n",
      "859: [D loss: 0.734785, acc: 0.511719]  [A loss: 1.059518, acc: 0.031250]\n",
      "860: [D loss: 0.698681, acc: 0.507812]  [A loss: 0.787837, acc: 0.335938]\n",
      "861: [D loss: 0.754176, acc: 0.500000]  [A loss: 1.019135, acc: 0.023438]\n",
      "862: [D loss: 0.695413, acc: 0.519531]  [A loss: 0.851540, acc: 0.171875]\n",
      "863: [D loss: 0.720470, acc: 0.488281]  [A loss: 1.006937, acc: 0.046875]\n",
      "864: [D loss: 0.702571, acc: 0.519531]  [A loss: 0.748479, acc: 0.406250]\n",
      "865: [D loss: 0.719053, acc: 0.500000]  [A loss: 1.026098, acc: 0.085938]\n",
      "866: [D loss: 0.708599, acc: 0.484375]  [A loss: 0.782361, acc: 0.328125]\n",
      "867: [D loss: 0.739961, acc: 0.488281]  [A loss: 0.847824, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868: [D loss: 0.704372, acc: 0.527344]  [A loss: 0.920008, acc: 0.125000]\n",
      "869: [D loss: 0.711200, acc: 0.542969]  [A loss: 0.909789, acc: 0.117188]\n",
      "870: [D loss: 0.711557, acc: 0.511719]  [A loss: 1.094824, acc: 0.046875]\n",
      "871: [D loss: 0.716648, acc: 0.500000]  [A loss: 0.760155, acc: 0.414062]\n",
      "872: [D loss: 0.720651, acc: 0.539062]  [A loss: 0.986546, acc: 0.109375]\n",
      "873: [D loss: 0.706643, acc: 0.531250]  [A loss: 0.863958, acc: 0.273438]\n",
      "874: [D loss: 0.689646, acc: 0.535156]  [A loss: 0.955192, acc: 0.093750]\n",
      "875: [D loss: 0.700102, acc: 0.503906]  [A loss: 0.901824, acc: 0.140625]\n",
      "876: [D loss: 0.704695, acc: 0.515625]  [A loss: 0.923629, acc: 0.140625]\n",
      "877: [D loss: 0.682131, acc: 0.597656]  [A loss: 0.877668, acc: 0.210938]\n",
      "878: [D loss: 0.709509, acc: 0.507812]  [A loss: 0.902435, acc: 0.140625]\n",
      "879: [D loss: 0.708672, acc: 0.488281]  [A loss: 0.933693, acc: 0.171875]\n",
      "880: [D loss: 0.706985, acc: 0.574219]  [A loss: 0.860530, acc: 0.179688]\n",
      "881: [D loss: 0.711894, acc: 0.523438]  [A loss: 1.014763, acc: 0.085938]\n",
      "882: [D loss: 0.694199, acc: 0.535156]  [A loss: 0.770438, acc: 0.335938]\n",
      "883: [D loss: 0.769692, acc: 0.507812]  [A loss: 1.134397, acc: 0.078125]\n",
      "884: [D loss: 0.764650, acc: 0.468750]  [A loss: 1.217734, acc: 0.000000]\n",
      "885: [D loss: 0.717397, acc: 0.492188]  [A loss: 0.778473, acc: 0.328125]\n",
      "886: [D loss: 0.729751, acc: 0.519531]  [A loss: 0.976962, acc: 0.070312]\n",
      "887: [D loss: 0.706962, acc: 0.523438]  [A loss: 0.812419, acc: 0.250000]\n",
      "888: [D loss: 0.721521, acc: 0.542969]  [A loss: 0.895254, acc: 0.195312]\n",
      "889: [D loss: 0.702409, acc: 0.539062]  [A loss: 0.837922, acc: 0.265625]\n",
      "890: [D loss: 0.723546, acc: 0.531250]  [A loss: 0.864697, acc: 0.148438]\n",
      "891: [D loss: 0.720885, acc: 0.460938]  [A loss: 0.815996, acc: 0.234375]\n",
      "892: [D loss: 0.719057, acc: 0.519531]  [A loss: 0.857081, acc: 0.179688]\n",
      "893: [D loss: 0.722046, acc: 0.507812]  [A loss: 0.896079, acc: 0.140625]\n",
      "894: [D loss: 0.722360, acc: 0.488281]  [A loss: 0.902174, acc: 0.109375]\n",
      "895: [D loss: 0.706835, acc: 0.492188]  [A loss: 0.807389, acc: 0.273438]\n",
      "896: [D loss: 0.724388, acc: 0.507812]  [A loss: 0.959478, acc: 0.093750]\n",
      "897: [D loss: 0.701135, acc: 0.496094]  [A loss: 0.753931, acc: 0.390625]\n",
      "898: [D loss: 0.746302, acc: 0.511719]  [A loss: 1.038115, acc: 0.023438]\n",
      "899: [D loss: 0.693691, acc: 0.515625]  [A loss: 0.784910, acc: 0.351562]\n",
      "900: [D loss: 0.804946, acc: 0.496094]  [A loss: 1.007666, acc: 0.070312]\n",
      "901: [D loss: 0.691803, acc: 0.546875]  [A loss: 0.752720, acc: 0.328125]\n",
      "902: [D loss: 0.750953, acc: 0.503906]  [A loss: 1.065356, acc: 0.023438]\n",
      "903: [D loss: 0.699664, acc: 0.519531]  [A loss: 0.743684, acc: 0.390625]\n",
      "904: [D loss: 0.738678, acc: 0.500000]  [A loss: 0.986610, acc: 0.125000]\n",
      "905: [D loss: 0.695158, acc: 0.531250]  [A loss: 0.777801, acc: 0.367188]\n",
      "906: [D loss: 0.724934, acc: 0.507812]  [A loss: 1.008049, acc: 0.046875]\n",
      "907: [D loss: 0.716207, acc: 0.484375]  [A loss: 0.828685, acc: 0.179688]\n",
      "908: [D loss: 0.689277, acc: 0.546875]  [A loss: 0.863991, acc: 0.187500]\n",
      "909: [D loss: 0.696233, acc: 0.535156]  [A loss: 0.781567, acc: 0.328125]\n",
      "910: [D loss: 0.706337, acc: 0.492188]  [A loss: 0.879481, acc: 0.148438]\n",
      "911: [D loss: 0.688300, acc: 0.562500]  [A loss: 0.867682, acc: 0.187500]\n",
      "912: [D loss: 0.711743, acc: 0.488281]  [A loss: 0.862820, acc: 0.164062]\n",
      "913: [D loss: 0.702220, acc: 0.519531]  [A loss: 0.847659, acc: 0.195312]\n",
      "914: [D loss: 0.707503, acc: 0.492188]  [A loss: 0.910287, acc: 0.171875]\n",
      "915: [D loss: 0.698814, acc: 0.542969]  [A loss: 0.918770, acc: 0.125000]\n",
      "916: [D loss: 0.726538, acc: 0.515625]  [A loss: 0.909677, acc: 0.187500]\n",
      "917: [D loss: 0.685851, acc: 0.558594]  [A loss: 0.854345, acc: 0.218750]\n",
      "918: [D loss: 0.735317, acc: 0.460938]  [A loss: 0.901307, acc: 0.109375]\n",
      "919: [D loss: 0.741853, acc: 0.464844]  [A loss: 1.090142, acc: 0.046875]\n",
      "920: [D loss: 0.699511, acc: 0.535156]  [A loss: 0.949763, acc: 0.250000]\n",
      "921: [D loss: 0.691277, acc: 0.531250]  [A loss: 1.126057, acc: 0.015625]\n",
      "922: [D loss: 0.713558, acc: 0.523438]  [A loss: 0.928652, acc: 0.101562]\n",
      "923: [D loss: 0.702109, acc: 0.531250]  [A loss: 0.827921, acc: 0.210938]\n",
      "924: [D loss: 0.717151, acc: 0.507812]  [A loss: 0.908948, acc: 0.093750]\n",
      "925: [D loss: 0.702957, acc: 0.546875]  [A loss: 0.869042, acc: 0.148438]\n",
      "926: [D loss: 0.719603, acc: 0.503906]  [A loss: 0.979489, acc: 0.085938]\n",
      "927: [D loss: 0.694254, acc: 0.539062]  [A loss: 0.786622, acc: 0.375000]\n",
      "928: [D loss: 0.755129, acc: 0.500000]  [A loss: 1.015382, acc: 0.039062]\n",
      "929: [D loss: 0.711275, acc: 0.496094]  [A loss: 0.853282, acc: 0.242188]\n",
      "930: [D loss: 0.716111, acc: 0.492188]  [A loss: 0.988568, acc: 0.085938]\n",
      "931: [D loss: 0.681404, acc: 0.554688]  [A loss: 0.794972, acc: 0.343750]\n",
      "932: [D loss: 0.784787, acc: 0.507812]  [A loss: 0.956384, acc: 0.054688]\n",
      "933: [D loss: 0.705649, acc: 0.492188]  [A loss: 0.908643, acc: 0.054688]\n",
      "934: [D loss: 0.716591, acc: 0.492188]  [A loss: 0.967419, acc: 0.085938]\n",
      "935: [D loss: 0.700180, acc: 0.539062]  [A loss: 0.811225, acc: 0.273438]\n",
      "936: [D loss: 0.735808, acc: 0.468750]  [A loss: 1.051736, acc: 0.078125]\n",
      "937: [D loss: 0.704888, acc: 0.507812]  [A loss: 0.839044, acc: 0.218750]\n",
      "938: [D loss: 0.727748, acc: 0.511719]  [A loss: 1.026922, acc: 0.101562]\n",
      "939: [D loss: 0.681443, acc: 0.570312]  [A loss: 0.726564, acc: 0.421875]\n",
      "940: [D loss: 0.724009, acc: 0.519531]  [A loss: 0.942540, acc: 0.132812]\n",
      "941: [D loss: 0.710087, acc: 0.523438]  [A loss: 0.856004, acc: 0.218750]\n",
      "942: [D loss: 0.704728, acc: 0.539062]  [A loss: 0.939680, acc: 0.117188]\n",
      "943: [D loss: 0.705894, acc: 0.519531]  [A loss: 0.876066, acc: 0.242188]\n",
      "944: [D loss: 0.709020, acc: 0.519531]  [A loss: 0.949212, acc: 0.125000]\n",
      "945: [D loss: 0.706673, acc: 0.523438]  [A loss: 0.802281, acc: 0.289062]\n",
      "946: [D loss: 0.730003, acc: 0.503906]  [A loss: 1.042091, acc: 0.046875]\n",
      "947: [D loss: 0.692979, acc: 0.539062]  [A loss: 0.709147, acc: 0.460938]\n",
      "948: [D loss: 0.750496, acc: 0.527344]  [A loss: 1.049125, acc: 0.062500]\n",
      "949: [D loss: 0.697879, acc: 0.519531]  [A loss: 0.820272, acc: 0.265625]\n",
      "950: [D loss: 0.715459, acc: 0.535156]  [A loss: 0.991292, acc: 0.039062]\n",
      "951: [D loss: 0.701631, acc: 0.507812]  [A loss: 0.713278, acc: 0.453125]\n",
      "952: [D loss: 0.797649, acc: 0.500000]  [A loss: 1.079375, acc: 0.023438]\n",
      "953: [D loss: 0.728823, acc: 0.515625]  [A loss: 0.904914, acc: 0.156250]\n",
      "954: [D loss: 0.716554, acc: 0.539062]  [A loss: 0.852421, acc: 0.187500]\n",
      "955: [D loss: 0.711459, acc: 0.496094]  [A loss: 0.824722, acc: 0.281250]\n",
      "956: [D loss: 0.714652, acc: 0.500000]  [A loss: 0.818343, acc: 0.179688]\n",
      "957: [D loss: 0.710792, acc: 0.519531]  [A loss: 0.872024, acc: 0.101562]\n",
      "958: [D loss: 0.710481, acc: 0.515625]  [A loss: 0.889160, acc: 0.101562]\n",
      "959: [D loss: 0.683852, acc: 0.562500]  [A loss: 0.851314, acc: 0.140625]\n",
      "960: [D loss: 0.700207, acc: 0.539062]  [A loss: 0.867558, acc: 0.164062]\n",
      "961: [D loss: 0.724499, acc: 0.480469]  [A loss: 0.943217, acc: 0.093750]\n",
      "962: [D loss: 0.723155, acc: 0.511719]  [A loss: 0.953677, acc: 0.031250]\n",
      "963: [D loss: 0.720393, acc: 0.464844]  [A loss: 0.846631, acc: 0.179688]\n",
      "964: [D loss: 0.710993, acc: 0.515625]  [A loss: 0.959035, acc: 0.125000]\n",
      "965: [D loss: 0.694083, acc: 0.550781]  [A loss: 0.826868, acc: 0.281250]\n",
      "966: [D loss: 0.720750, acc: 0.511719]  [A loss: 0.949213, acc: 0.140625]\n",
      "967: [D loss: 0.707282, acc: 0.484375]  [A loss: 0.994462, acc: 0.054688]\n",
      "968: [D loss: 0.687993, acc: 0.566406]  [A loss: 0.800667, acc: 0.320312]\n",
      "969: [D loss: 0.720641, acc: 0.511719]  [A loss: 0.939430, acc: 0.101562]\n",
      "970: [D loss: 0.693041, acc: 0.554688]  [A loss: 0.830916, acc: 0.250000]\n",
      "971: [D loss: 0.693419, acc: 0.542969]  [A loss: 0.878253, acc: 0.179688]\n",
      "972: [D loss: 0.703514, acc: 0.546875]  [A loss: 0.867407, acc: 0.171875]\n",
      "973: [D loss: 0.699761, acc: 0.507812]  [A loss: 0.991051, acc: 0.078125]\n",
      "974: [D loss: 0.710683, acc: 0.484375]  [A loss: 0.826082, acc: 0.234375]\n",
      "975: [D loss: 0.703411, acc: 0.542969]  [A loss: 0.970590, acc: 0.117188]\n",
      "976: [D loss: 0.718219, acc: 0.507812]  [A loss: 0.927521, acc: 0.109375]\n",
      "977: [D loss: 0.681528, acc: 0.578125]  [A loss: 0.984553, acc: 0.085938]\n",
      "978: [D loss: 0.682704, acc: 0.589844]  [A loss: 1.022199, acc: 0.078125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979: [D loss: 0.787863, acc: 0.507812]  [A loss: 1.114728, acc: 0.023438]\n",
      "980: [D loss: 0.708338, acc: 0.484375]  [A loss: 0.847922, acc: 0.171875]\n",
      "981: [D loss: 0.746037, acc: 0.468750]  [A loss: 1.213970, acc: 0.007812]\n",
      "982: [D loss: 0.700380, acc: 0.519531]  [A loss: 0.682686, acc: 0.492188]\n",
      "983: [D loss: 0.790000, acc: 0.496094]  [A loss: 1.171542, acc: 0.015625]\n",
      "984: [D loss: 0.711893, acc: 0.535156]  [A loss: 0.748947, acc: 0.398438]\n",
      "985: [D loss: 0.738160, acc: 0.515625]  [A loss: 0.931946, acc: 0.125000]\n",
      "986: [D loss: 0.710318, acc: 0.511719]  [A loss: 0.933005, acc: 0.078125]\n",
      "987: [D loss: 0.694620, acc: 0.531250]  [A loss: 0.802888, acc: 0.289062]\n",
      "988: [D loss: 0.719710, acc: 0.539062]  [A loss: 0.822587, acc: 0.242188]\n",
      "989: [D loss: 0.708716, acc: 0.480469]  [A loss: 0.783071, acc: 0.320312]\n",
      "990: [D loss: 0.703939, acc: 0.515625]  [A loss: 0.868922, acc: 0.148438]\n",
      "991: [D loss: 0.707802, acc: 0.492188]  [A loss: 0.824824, acc: 0.328125]\n",
      "992: [D loss: 0.715902, acc: 0.511719]  [A loss: 0.909796, acc: 0.125000]\n",
      "993: [D loss: 0.687921, acc: 0.542969]  [A loss: 0.808412, acc: 0.265625]\n",
      "994: [D loss: 0.742959, acc: 0.503906]  [A loss: 0.891742, acc: 0.070312]\n",
      "995: [D loss: 0.691403, acc: 0.527344]  [A loss: 0.819054, acc: 0.242188]\n",
      "996: [D loss: 0.718870, acc: 0.507812]  [A loss: 0.978837, acc: 0.117188]\n",
      "997: [D loss: 0.713841, acc: 0.523438]  [A loss: 0.851075, acc: 0.218750]\n",
      "998: [D loss: 0.698825, acc: 0.515625]  [A loss: 0.794401, acc: 0.250000]\n",
      "999: [D loss: 0.727496, acc: 0.484375]  [A loss: 0.969699, acc: 0.062500]\n",
      "1000: [D loss: 0.699273, acc: 0.519531]  [A loss: 0.728536, acc: 0.445312]\n",
      "1001: [D loss: 0.734460, acc: 0.492188]  [A loss: 1.061001, acc: 0.062500]\n",
      "1002: [D loss: 0.714742, acc: 0.539062]  [A loss: 1.065521, acc: 0.062500]\n",
      "1003: [D loss: 0.690763, acc: 0.554688]  [A loss: 0.730520, acc: 0.414062]\n",
      "1004: [D loss: 0.723580, acc: 0.515625]  [A loss: 1.026407, acc: 0.046875]\n",
      "1005: [D loss: 0.697049, acc: 0.546875]  [A loss: 0.716433, acc: 0.492188]\n",
      "1006: [D loss: 0.741356, acc: 0.484375]  [A loss: 0.954621, acc: 0.046875]\n",
      "1007: [D loss: 0.701133, acc: 0.511719]  [A loss: 0.738027, acc: 0.437500]\n",
      "1008: [D loss: 0.710018, acc: 0.507812]  [A loss: 0.914569, acc: 0.117188]\n",
      "1009: [D loss: 0.689574, acc: 0.562500]  [A loss: 0.872737, acc: 0.203125]\n",
      "1010: [D loss: 0.691171, acc: 0.562500]  [A loss: 0.811434, acc: 0.304688]\n",
      "1011: [D loss: 0.703383, acc: 0.523438]  [A loss: 0.860192, acc: 0.171875]\n",
      "1012: [D loss: 0.720262, acc: 0.523438]  [A loss: 0.842912, acc: 0.226562]\n",
      "1013: [D loss: 0.700481, acc: 0.531250]  [A loss: 0.866283, acc: 0.148438]\n",
      "1014: [D loss: 0.702510, acc: 0.523438]  [A loss: 0.960783, acc: 0.117188]\n",
      "1015: [D loss: 0.711457, acc: 0.519531]  [A loss: 0.911769, acc: 0.109375]\n",
      "1016: [D loss: 0.699357, acc: 0.539062]  [A loss: 0.900296, acc: 0.164062]\n",
      "1017: [D loss: 0.690578, acc: 0.535156]  [A loss: 0.904792, acc: 0.164062]\n",
      "1018: [D loss: 0.699322, acc: 0.546875]  [A loss: 0.945398, acc: 0.148438]\n",
      "1019: [D loss: 0.714128, acc: 0.523438]  [A loss: 1.125128, acc: 0.078125]\n",
      "1020: [D loss: 0.698763, acc: 0.523438]  [A loss: 0.911964, acc: 0.164062]\n",
      "1021: [D loss: 0.779241, acc: 0.519531]  [A loss: 1.013397, acc: 0.062500]\n",
      "1022: [D loss: 0.725487, acc: 0.441406]  [A loss: 0.848443, acc: 0.148438]\n",
      "1023: [D loss: 0.705079, acc: 0.507812]  [A loss: 0.980612, acc: 0.039062]\n",
      "1024: [D loss: 0.717190, acc: 0.464844]  [A loss: 0.799570, acc: 0.304688]\n",
      "1025: [D loss: 0.735766, acc: 0.523438]  [A loss: 1.146112, acc: 0.007812]\n",
      "1026: [D loss: 0.682369, acc: 0.562500]  [A loss: 0.708939, acc: 0.507812]\n",
      "1027: [D loss: 0.707594, acc: 0.515625]  [A loss: 0.961694, acc: 0.117188]\n",
      "1028: [D loss: 0.696113, acc: 0.546875]  [A loss: 0.792957, acc: 0.296875]\n",
      "1029: [D loss: 0.706250, acc: 0.542969]  [A loss: 0.877428, acc: 0.171875]\n",
      "1030: [D loss: 0.703858, acc: 0.542969]  [A loss: 0.788612, acc: 0.343750]\n",
      "1031: [D loss: 0.714928, acc: 0.527344]  [A loss: 0.974919, acc: 0.078125]\n",
      "1032: [D loss: 0.712426, acc: 0.453125]  [A loss: 0.768376, acc: 0.375000]\n",
      "1033: [D loss: 0.726223, acc: 0.527344]  [A loss: 1.061600, acc: 0.031250]\n",
      "1034: [D loss: 0.688626, acc: 0.519531]  [A loss: 0.798887, acc: 0.273438]\n",
      "1035: [D loss: 0.742883, acc: 0.488281]  [A loss: 1.119412, acc: 0.054688]\n",
      "1036: [D loss: 0.731397, acc: 0.460938]  [A loss: 0.808028, acc: 0.296875]\n",
      "1037: [D loss: 0.745050, acc: 0.476562]  [A loss: 0.962434, acc: 0.039062]\n",
      "1038: [D loss: 0.695283, acc: 0.554688]  [A loss: 0.760239, acc: 0.335938]\n",
      "1039: [D loss: 0.721535, acc: 0.468750]  [A loss: 0.981636, acc: 0.062500]\n",
      "1040: [D loss: 0.688707, acc: 0.511719]  [A loss: 0.744053, acc: 0.414062]\n",
      "1041: [D loss: 0.729231, acc: 0.496094]  [A loss: 0.963236, acc: 0.078125]\n",
      "1042: [D loss: 0.683630, acc: 0.539062]  [A loss: 0.746772, acc: 0.398438]\n",
      "1043: [D loss: 0.727257, acc: 0.503906]  [A loss: 0.896731, acc: 0.132812]\n",
      "1044: [D loss: 0.685207, acc: 0.546875]  [A loss: 0.861554, acc: 0.179688]\n",
      "1045: [D loss: 0.705921, acc: 0.523438]  [A loss: 0.866951, acc: 0.234375]\n",
      "1046: [D loss: 0.699011, acc: 0.562500]  [A loss: 0.817608, acc: 0.257812]\n",
      "1047: [D loss: 0.706415, acc: 0.527344]  [A loss: 0.929157, acc: 0.109375]\n",
      "1048: [D loss: 0.700732, acc: 0.562500]  [A loss: 0.938547, acc: 0.109375]\n",
      "1049: [D loss: 0.701679, acc: 0.550781]  [A loss: 0.903155, acc: 0.117188]\n",
      "1050: [D loss: 0.683783, acc: 0.570312]  [A loss: 0.878981, acc: 0.164062]\n",
      "1051: [D loss: 0.703837, acc: 0.535156]  [A loss: 1.025511, acc: 0.039062]\n",
      "1052: [D loss: 0.690917, acc: 0.500000]  [A loss: 0.883217, acc: 0.156250]\n",
      "1053: [D loss: 0.707706, acc: 0.527344]  [A loss: 1.221908, acc: 0.023438]\n",
      "1054: [D loss: 0.684993, acc: 0.542969]  [A loss: 1.330221, acc: 0.039062]\n",
      "1055: [D loss: 0.651191, acc: 0.644531]  [A loss: 0.629409, acc: 0.648438]\n",
      "1056: [D loss: 0.889644, acc: 0.500000]  [A loss: 1.342573, acc: 0.000000]\n",
      "1057: [D loss: 0.718411, acc: 0.449219]  [A loss: 0.649776, acc: 0.570312]\n",
      "1058: [D loss: 0.791219, acc: 0.500000]  [A loss: 1.222469, acc: 0.007812]\n",
      "1059: [D loss: 0.699748, acc: 0.507812]  [A loss: 0.805642, acc: 0.296875]\n",
      "1060: [D loss: 0.741789, acc: 0.453125]  [A loss: 0.798257, acc: 0.304688]\n",
      "1061: [D loss: 0.680972, acc: 0.562500]  [A loss: 0.780109, acc: 0.328125]\n",
      "1062: [D loss: 0.706128, acc: 0.511719]  [A loss: 0.765051, acc: 0.281250]\n",
      "1063: [D loss: 0.689262, acc: 0.566406]  [A loss: 0.825102, acc: 0.273438]\n",
      "1064: [D loss: 0.710336, acc: 0.492188]  [A loss: 0.821210, acc: 0.203125]\n",
      "1065: [D loss: 0.720707, acc: 0.519531]  [A loss: 0.797202, acc: 0.281250]\n",
      "1066: [D loss: 0.693106, acc: 0.542969]  [A loss: 0.759069, acc: 0.335938]\n",
      "1067: [D loss: 0.706467, acc: 0.519531]  [A loss: 0.797300, acc: 0.289062]\n",
      "1068: [D loss: 0.739308, acc: 0.468750]  [A loss: 0.903230, acc: 0.140625]\n",
      "1069: [D loss: 0.692211, acc: 0.570312]  [A loss: 0.771784, acc: 0.312500]\n",
      "1070: [D loss: 0.742041, acc: 0.476562]  [A loss: 0.891488, acc: 0.140625]\n",
      "1071: [D loss: 0.709124, acc: 0.542969]  [A loss: 0.897246, acc: 0.148438]\n",
      "1072: [D loss: 0.677339, acc: 0.566406]  [A loss: 0.799847, acc: 0.320312]\n",
      "1073: [D loss: 0.707211, acc: 0.535156]  [A loss: 0.887670, acc: 0.171875]\n",
      "1074: [D loss: 0.701165, acc: 0.488281]  [A loss: 0.897173, acc: 0.187500]\n",
      "1075: [D loss: 0.694697, acc: 0.515625]  [A loss: 0.836509, acc: 0.304688]\n",
      "1076: [D loss: 0.730990, acc: 0.484375]  [A loss: 0.991730, acc: 0.101562]\n",
      "1077: [D loss: 0.724666, acc: 0.460938]  [A loss: 0.707222, acc: 0.500000]\n",
      "1078: [D loss: 0.751753, acc: 0.492188]  [A loss: 0.994943, acc: 0.078125]\n",
      "1079: [D loss: 0.698638, acc: 0.531250]  [A loss: 0.906640, acc: 0.203125]\n",
      "1080: [D loss: 0.731210, acc: 0.480469]  [A loss: 1.002609, acc: 0.046875]\n",
      "1081: [D loss: 0.685559, acc: 0.570312]  [A loss: 0.819531, acc: 0.242188]\n",
      "1082: [D loss: 0.705587, acc: 0.503906]  [A loss: 0.861958, acc: 0.203125]\n",
      "1083: [D loss: 0.690727, acc: 0.539062]  [A loss: 0.782207, acc: 0.382812]\n",
      "1084: [D loss: 0.720173, acc: 0.500000]  [A loss: 0.899073, acc: 0.148438]\n",
      "1085: [D loss: 0.685930, acc: 0.562500]  [A loss: 0.724291, acc: 0.476562]\n",
      "1086: [D loss: 0.726052, acc: 0.527344]  [A loss: 1.066296, acc: 0.085938]\n",
      "1087: [D loss: 0.725201, acc: 0.500000]  [A loss: 0.849577, acc: 0.203125]\n",
      "1088: [D loss: 0.698816, acc: 0.515625]  [A loss: 0.883935, acc: 0.164062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089: [D loss: 0.704357, acc: 0.527344]  [A loss: 0.885119, acc: 0.148438]\n",
      "1090: [D loss: 0.685502, acc: 0.539062]  [A loss: 0.813450, acc: 0.234375]\n",
      "1091: [D loss: 0.728933, acc: 0.500000]  [A loss: 1.063168, acc: 0.062500]\n",
      "1092: [D loss: 0.702126, acc: 0.531250]  [A loss: 0.811259, acc: 0.250000]\n",
      "1093: [D loss: 0.728402, acc: 0.519531]  [A loss: 1.006306, acc: 0.070312]\n",
      "1094: [D loss: 0.690523, acc: 0.527344]  [A loss: 0.798403, acc: 0.335938]\n",
      "1095: [D loss: 0.732419, acc: 0.515625]  [A loss: 1.055282, acc: 0.023438]\n",
      "1096: [D loss: 0.706267, acc: 0.515625]  [A loss: 0.866564, acc: 0.242188]\n",
      "1097: [D loss: 0.720572, acc: 0.527344]  [A loss: 1.056913, acc: 0.062500]\n",
      "1098: [D loss: 0.686615, acc: 0.574219]  [A loss: 0.762371, acc: 0.367188]\n",
      "1099: [D loss: 0.741541, acc: 0.496094]  [A loss: 0.941560, acc: 0.109375]\n",
      "1100: [D loss: 0.699641, acc: 0.535156]  [A loss: 0.786447, acc: 0.281250]\n",
      "1101: [D loss: 0.720745, acc: 0.492188]  [A loss: 0.926687, acc: 0.148438]\n",
      "1102: [D loss: 0.678881, acc: 0.566406]  [A loss: 0.754850, acc: 0.382812]\n",
      "1103: [D loss: 0.741598, acc: 0.492188]  [A loss: 0.899457, acc: 0.117188]\n",
      "1104: [D loss: 0.694755, acc: 0.492188]  [A loss: 0.829643, acc: 0.250000]\n",
      "1105: [D loss: 0.732336, acc: 0.511719]  [A loss: 1.106923, acc: 0.054688]\n",
      "1106: [D loss: 0.712314, acc: 0.500000]  [A loss: 0.793887, acc: 0.257812]\n",
      "1107: [D loss: 0.714153, acc: 0.531250]  [A loss: 0.916898, acc: 0.085938]\n",
      "1108: [D loss: 0.688715, acc: 0.554688]  [A loss: 0.789719, acc: 0.281250]\n",
      "1109: [D loss: 0.722213, acc: 0.488281]  [A loss: 0.999227, acc: 0.070312]\n",
      "1110: [D loss: 0.707586, acc: 0.527344]  [A loss: 0.782341, acc: 0.328125]\n",
      "1111: [D loss: 0.734553, acc: 0.496094]  [A loss: 1.149254, acc: 0.000000]\n",
      "1112: [D loss: 0.701549, acc: 0.515625]  [A loss: 0.804116, acc: 0.273438]\n",
      "1113: [D loss: 0.733651, acc: 0.507812]  [A loss: 0.878433, acc: 0.125000]\n",
      "1114: [D loss: 0.694637, acc: 0.523438]  [A loss: 0.794306, acc: 0.273438]\n",
      "1115: [D loss: 0.716155, acc: 0.519531]  [A loss: 1.028556, acc: 0.023438]\n",
      "1116: [D loss: 0.680872, acc: 0.566406]  [A loss: 0.749011, acc: 0.460938]\n",
      "1117: [D loss: 0.760541, acc: 0.488281]  [A loss: 1.016211, acc: 0.039062]\n",
      "1118: [D loss: 0.720479, acc: 0.464844]  [A loss: 0.941416, acc: 0.070312]\n",
      "1119: [D loss: 0.701536, acc: 0.492188]  [A loss: 0.817263, acc: 0.210938]\n",
      "1120: [D loss: 0.717426, acc: 0.519531]  [A loss: 0.956564, acc: 0.093750]\n",
      "1121: [D loss: 0.695423, acc: 0.511719]  [A loss: 0.816657, acc: 0.218750]\n",
      "1122: [D loss: 0.712009, acc: 0.503906]  [A loss: 1.035708, acc: 0.007812]\n",
      "1123: [D loss: 0.708319, acc: 0.500000]  [A loss: 0.772877, acc: 0.335938]\n",
      "1124: [D loss: 0.731437, acc: 0.496094]  [A loss: 1.094444, acc: 0.007812]\n",
      "1125: [D loss: 0.686709, acc: 0.589844]  [A loss: 0.737954, acc: 0.414062]\n",
      "1126: [D loss: 0.723747, acc: 0.519531]  [A loss: 0.995376, acc: 0.101562]\n",
      "1127: [D loss: 0.695317, acc: 0.507812]  [A loss: 0.756667, acc: 0.375000]\n",
      "1128: [D loss: 0.753085, acc: 0.500000]  [A loss: 1.124178, acc: 0.023438]\n",
      "1129: [D loss: 0.698728, acc: 0.519531]  [A loss: 0.692160, acc: 0.531250]\n",
      "1130: [D loss: 0.772005, acc: 0.507812]  [A loss: 1.089748, acc: 0.015625]\n",
      "1131: [D loss: 0.691761, acc: 0.558594]  [A loss: 0.698569, acc: 0.531250]\n",
      "1132: [D loss: 0.888306, acc: 0.453125]  [A loss: 0.895884, acc: 0.054688]\n",
      "1133: [D loss: 0.712599, acc: 0.515625]  [A loss: 0.909042, acc: 0.078125]\n",
      "1134: [D loss: 0.712228, acc: 0.484375]  [A loss: 0.804307, acc: 0.179688]\n",
      "1135: [D loss: 0.723513, acc: 0.480469]  [A loss: 0.934677, acc: 0.109375]\n",
      "1136: [D loss: 0.686914, acc: 0.554688]  [A loss: 0.758427, acc: 0.257812]\n",
      "1137: [D loss: 0.732531, acc: 0.492188]  [A loss: 0.989325, acc: 0.046875]\n",
      "1138: [D loss: 0.675360, acc: 0.582031]  [A loss: 0.842313, acc: 0.218750]\n",
      "1139: [D loss: 0.709094, acc: 0.484375]  [A loss: 0.903515, acc: 0.125000]\n",
      "1140: [D loss: 0.710157, acc: 0.488281]  [A loss: 0.813039, acc: 0.273438]\n",
      "1141: [D loss: 0.698025, acc: 0.539062]  [A loss: 0.784062, acc: 0.304688]\n",
      "1142: [D loss: 0.727426, acc: 0.457031]  [A loss: 0.908086, acc: 0.125000]\n",
      "1143: [D loss: 0.704649, acc: 0.511719]  [A loss: 0.820086, acc: 0.312500]\n",
      "1144: [D loss: 0.722545, acc: 0.472656]  [A loss: 0.946964, acc: 0.070312]\n",
      "1145: [D loss: 0.708148, acc: 0.527344]  [A loss: 0.859146, acc: 0.226562]\n",
      "1146: [D loss: 0.714960, acc: 0.570312]  [A loss: 0.816438, acc: 0.226562]\n",
      "1147: [D loss: 0.726371, acc: 0.484375]  [A loss: 0.908423, acc: 0.125000]\n",
      "1148: [D loss: 0.703885, acc: 0.539062]  [A loss: 0.795353, acc: 0.312500]\n",
      "1149: [D loss: 0.702049, acc: 0.523438]  [A loss: 0.909904, acc: 0.117188]\n",
      "1150: [D loss: 0.705842, acc: 0.496094]  [A loss: 0.766932, acc: 0.351562]\n",
      "1151: [D loss: 0.705149, acc: 0.503906]  [A loss: 1.005950, acc: 0.046875]\n",
      "1152: [D loss: 0.715204, acc: 0.472656]  [A loss: 0.833095, acc: 0.281250]\n",
      "1153: [D loss: 0.738560, acc: 0.476562]  [A loss: 1.091474, acc: 0.039062]\n",
      "1154: [D loss: 0.700868, acc: 0.492188]  [A loss: 0.762136, acc: 0.359375]\n",
      "1155: [D loss: 0.730922, acc: 0.503906]  [A loss: 0.930754, acc: 0.085938]\n",
      "1156: [D loss: 0.678641, acc: 0.570312]  [A loss: 0.793267, acc: 0.328125]\n",
      "1157: [D loss: 0.709671, acc: 0.519531]  [A loss: 1.089913, acc: 0.046875]\n",
      "1158: [D loss: 0.702436, acc: 0.531250]  [A loss: 0.887966, acc: 0.273438]\n",
      "1159: [D loss: 0.698856, acc: 0.535156]  [A loss: 1.007763, acc: 0.101562]\n",
      "1160: [D loss: 0.704252, acc: 0.507812]  [A loss: 0.844916, acc: 0.234375]\n",
      "1161: [D loss: 0.728072, acc: 0.503906]  [A loss: 0.851570, acc: 0.210938]\n",
      "1162: [D loss: 0.745121, acc: 0.515625]  [A loss: 1.223597, acc: 0.000000]\n",
      "1163: [D loss: 0.715974, acc: 0.496094]  [A loss: 0.959044, acc: 0.226562]\n",
      "1164: [D loss: 0.670880, acc: 0.554688]  [A loss: 1.005922, acc: 0.101562]\n",
      "1165: [D loss: 0.725503, acc: 0.511719]  [A loss: 1.070369, acc: 0.046875]\n",
      "1166: [D loss: 0.701498, acc: 0.519531]  [A loss: 0.711419, acc: 0.460938]\n",
      "1167: [D loss: 0.762425, acc: 0.488281]  [A loss: 1.073804, acc: 0.023438]\n",
      "1168: [D loss: 0.701070, acc: 0.496094]  [A loss: 0.746206, acc: 0.382812]\n",
      "1169: [D loss: 0.726522, acc: 0.515625]  [A loss: 1.053941, acc: 0.054688]\n",
      "1170: [D loss: 0.683830, acc: 0.578125]  [A loss: 0.764038, acc: 0.390625]\n",
      "1171: [D loss: 0.705916, acc: 0.503906]  [A loss: 0.879352, acc: 0.164062]\n",
      "1172: [D loss: 0.704748, acc: 0.507812]  [A loss: 0.865979, acc: 0.132812]\n",
      "1173: [D loss: 0.693910, acc: 0.562500]  [A loss: 0.839912, acc: 0.203125]\n",
      "1174: [D loss: 0.687132, acc: 0.574219]  [A loss: 0.863793, acc: 0.234375]\n",
      "1175: [D loss: 0.698426, acc: 0.480469]  [A loss: 0.906216, acc: 0.148438]\n",
      "1176: [D loss: 0.702027, acc: 0.523438]  [A loss: 0.876842, acc: 0.156250]\n",
      "1177: [D loss: 0.698800, acc: 0.558594]  [A loss: 0.942313, acc: 0.101562]\n",
      "1178: [D loss: 0.693518, acc: 0.511719]  [A loss: 0.829573, acc: 0.250000]\n",
      "1179: [D loss: 0.707107, acc: 0.554688]  [A loss: 1.010083, acc: 0.078125]\n",
      "1180: [D loss: 0.678554, acc: 0.546875]  [A loss: 0.712885, acc: 0.460938]\n",
      "1181: [D loss: 0.797011, acc: 0.519531]  [A loss: 1.062036, acc: 0.078125]\n",
      "1182: [D loss: 0.691463, acc: 0.523438]  [A loss: 1.068419, acc: 0.046875]\n",
      "1183: [D loss: 0.711575, acc: 0.511719]  [A loss: 0.880053, acc: 0.210938]\n",
      "1184: [D loss: 0.716513, acc: 0.488281]  [A loss: 1.027209, acc: 0.078125]\n",
      "1185: [D loss: 0.705389, acc: 0.503906]  [A loss: 0.721581, acc: 0.398438]\n",
      "1186: [D loss: 0.679665, acc: 0.554688]  [A loss: 0.897915, acc: 0.148438]\n",
      "1187: [D loss: 0.686869, acc: 0.535156]  [A loss: 0.766772, acc: 0.390625]\n",
      "1188: [D loss: 0.734340, acc: 0.480469]  [A loss: 0.926079, acc: 0.148438]\n",
      "1189: [D loss: 0.707202, acc: 0.519531]  [A loss: 0.928739, acc: 0.117188]\n",
      "1190: [D loss: 0.716335, acc: 0.472656]  [A loss: 0.851109, acc: 0.195312]\n",
      "1191: [D loss: 0.720614, acc: 0.472656]  [A loss: 0.982640, acc: 0.062500]\n",
      "1192: [D loss: 0.667426, acc: 0.578125]  [A loss: 0.799272, acc: 0.289062]\n",
      "1193: [D loss: 0.702253, acc: 0.519531]  [A loss: 0.989803, acc: 0.140625]\n",
      "1194: [D loss: 0.712915, acc: 0.527344]  [A loss: 0.835716, acc: 0.210938]\n",
      "1195: [D loss: 0.734948, acc: 0.492188]  [A loss: 1.110697, acc: 0.039062]\n",
      "1196: [D loss: 0.677285, acc: 0.566406]  [A loss: 0.851833, acc: 0.265625]\n",
      "1197: [D loss: 0.732857, acc: 0.531250]  [A loss: 1.024033, acc: 0.054688]\n",
      "1198: [D loss: 0.704140, acc: 0.503906]  [A loss: 0.895596, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199: [D loss: 0.729306, acc: 0.500000]  [A loss: 1.028865, acc: 0.046875]\n",
      "1200: [D loss: 0.712201, acc: 0.484375]  [A loss: 0.731772, acc: 0.390625]\n",
      "1201: [D loss: 0.717203, acc: 0.539062]  [A loss: 1.057060, acc: 0.078125]\n",
      "1202: [D loss: 0.678862, acc: 0.574219]  [A loss: 0.783074, acc: 0.335938]\n",
      "1203: [D loss: 0.746181, acc: 0.480469]  [A loss: 1.064904, acc: 0.070312]\n",
      "1204: [D loss: 0.730312, acc: 0.503906]  [A loss: 0.886808, acc: 0.250000]\n",
      "1205: [D loss: 0.721153, acc: 0.484375]  [A loss: 0.906374, acc: 0.132812]\n",
      "1206: [D loss: 0.690414, acc: 0.566406]  [A loss: 0.858329, acc: 0.164062]\n",
      "1207: [D loss: 0.696350, acc: 0.558594]  [A loss: 0.917265, acc: 0.187500]\n",
      "1208: [D loss: 0.681155, acc: 0.562500]  [A loss: 0.812541, acc: 0.328125]\n",
      "1209: [D loss: 0.707779, acc: 0.531250]  [A loss: 0.965559, acc: 0.140625]\n",
      "1210: [D loss: 0.717369, acc: 0.511719]  [A loss: 0.772054, acc: 0.351562]\n",
      "1211: [D loss: 0.780572, acc: 0.500000]  [A loss: 1.303472, acc: 0.023438]\n",
      "1212: [D loss: 0.753554, acc: 0.515625]  [A loss: 0.918941, acc: 0.187500]\n",
      "1213: [D loss: 0.738906, acc: 0.578125]  [A loss: 1.584947, acc: 0.000000]\n",
      "1214: [D loss: 0.681565, acc: 0.578125]  [A loss: 0.707280, acc: 0.484375]\n",
      "1215: [D loss: 0.772830, acc: 0.480469]  [A loss: 1.044872, acc: 0.015625]\n",
      "1216: [D loss: 0.704153, acc: 0.523438]  [A loss: 0.773987, acc: 0.312500]\n",
      "1217: [D loss: 0.699642, acc: 0.511719]  [A loss: 0.879631, acc: 0.195312]\n",
      "1218: [D loss: 0.696286, acc: 0.574219]  [A loss: 0.794322, acc: 0.296875]\n",
      "1219: [D loss: 0.706323, acc: 0.527344]  [A loss: 0.850713, acc: 0.156250]\n",
      "1220: [D loss: 0.701306, acc: 0.523438]  [A loss: 0.890411, acc: 0.101562]\n",
      "1221: [D loss: 0.700262, acc: 0.535156]  [A loss: 0.855084, acc: 0.226562]\n",
      "1222: [D loss: 0.703901, acc: 0.527344]  [A loss: 0.977385, acc: 0.078125]\n",
      "1223: [D loss: 0.672656, acc: 0.574219]  [A loss: 0.795143, acc: 0.312500]\n",
      "1224: [D loss: 0.714859, acc: 0.503906]  [A loss: 0.994774, acc: 0.046875]\n",
      "1225: [D loss: 0.682973, acc: 0.585938]  [A loss: 0.761479, acc: 0.382812]\n",
      "1226: [D loss: 0.705405, acc: 0.523438]  [A loss: 0.995252, acc: 0.070312]\n",
      "1227: [D loss: 0.680481, acc: 0.566406]  [A loss: 0.767985, acc: 0.398438]\n",
      "1228: [D loss: 0.678931, acc: 0.585938]  [A loss: 1.006491, acc: 0.101562]\n",
      "1229: [D loss: 0.677576, acc: 0.570312]  [A loss: 0.819774, acc: 0.320312]\n",
      "1230: [D loss: 0.731191, acc: 0.503906]  [A loss: 1.109962, acc: 0.039062]\n",
      "1231: [D loss: 0.725164, acc: 0.468750]  [A loss: 0.705316, acc: 0.531250]\n",
      "1232: [D loss: 0.722531, acc: 0.484375]  [A loss: 1.002239, acc: 0.046875]\n",
      "1233: [D loss: 0.688513, acc: 0.511719]  [A loss: 0.813907, acc: 0.312500]\n",
      "1234: [D loss: 0.689419, acc: 0.574219]  [A loss: 1.003967, acc: 0.078125]\n",
      "1235: [D loss: 0.712382, acc: 0.511719]  [A loss: 0.866614, acc: 0.164062]\n",
      "1236: [D loss: 0.694923, acc: 0.539062]  [A loss: 0.964601, acc: 0.132812]\n",
      "1237: [D loss: 0.701622, acc: 0.511719]  [A loss: 0.853719, acc: 0.187500]\n",
      "1238: [D loss: 0.702632, acc: 0.519531]  [A loss: 0.932959, acc: 0.132812]\n",
      "1239: [D loss: 0.705027, acc: 0.519531]  [A loss: 0.805876, acc: 0.296875]\n",
      "1240: [D loss: 0.692368, acc: 0.570312]  [A loss: 1.018260, acc: 0.140625]\n",
      "1241: [D loss: 0.736727, acc: 0.468750]  [A loss: 0.880147, acc: 0.140625]\n",
      "1242: [D loss: 0.711103, acc: 0.511719]  [A loss: 1.032115, acc: 0.046875]\n",
      "1243: [D loss: 0.691564, acc: 0.527344]  [A loss: 0.688616, acc: 0.500000]\n",
      "1244: [D loss: 0.759547, acc: 0.500000]  [A loss: 1.188344, acc: 0.023438]\n",
      "1245: [D loss: 0.712606, acc: 0.457031]  [A loss: 0.853333, acc: 0.234375]\n",
      "1246: [D loss: 0.743489, acc: 0.492188]  [A loss: 1.041987, acc: 0.046875]\n",
      "1247: [D loss: 0.683154, acc: 0.578125]  [A loss: 0.717559, acc: 0.476562]\n",
      "1248: [D loss: 0.724859, acc: 0.523438]  [A loss: 0.904499, acc: 0.171875]\n",
      "1249: [D loss: 0.687660, acc: 0.562500]  [A loss: 0.757442, acc: 0.414062]\n",
      "1250: [D loss: 0.722818, acc: 0.515625]  [A loss: 1.011918, acc: 0.101562]\n",
      "1251: [D loss: 0.694907, acc: 0.511719]  [A loss: 0.786687, acc: 0.367188]\n",
      "1252: [D loss: 0.727264, acc: 0.515625]  [A loss: 0.930418, acc: 0.093750]\n",
      "1253: [D loss: 0.702426, acc: 0.539062]  [A loss: 0.825603, acc: 0.250000]\n",
      "1254: [D loss: 0.717830, acc: 0.515625]  [A loss: 0.966738, acc: 0.039062]\n",
      "1255: [D loss: 0.702024, acc: 0.527344]  [A loss: 0.871011, acc: 0.179688]\n",
      "1256: [D loss: 0.705028, acc: 0.539062]  [A loss: 1.006024, acc: 0.148438]\n",
      "1257: [D loss: 0.698357, acc: 0.531250]  [A loss: 0.793760, acc: 0.304688]\n",
      "1258: [D loss: 0.742208, acc: 0.496094]  [A loss: 1.046555, acc: 0.039062]\n",
      "1259: [D loss: 0.692307, acc: 0.542969]  [A loss: 0.729493, acc: 0.398438]\n",
      "1260: [D loss: 0.772744, acc: 0.511719]  [A loss: 1.079317, acc: 0.039062]\n",
      "1261: [D loss: 0.706649, acc: 0.515625]  [A loss: 0.886585, acc: 0.187500]\n",
      "1262: [D loss: 0.713330, acc: 0.519531]  [A loss: 0.919254, acc: 0.164062]\n",
      "1263: [D loss: 0.722895, acc: 0.484375]  [A loss: 0.960766, acc: 0.054688]\n",
      "1264: [D loss: 0.694177, acc: 0.566406]  [A loss: 0.829077, acc: 0.242188]\n",
      "1265: [D loss: 0.717788, acc: 0.515625]  [A loss: 0.967451, acc: 0.093750]\n",
      "1266: [D loss: 0.703186, acc: 0.550781]  [A loss: 0.810891, acc: 0.218750]\n",
      "1267: [D loss: 0.709607, acc: 0.500000]  [A loss: 0.997044, acc: 0.031250]\n",
      "1268: [D loss: 0.688806, acc: 0.535156]  [A loss: 0.773284, acc: 0.359375]\n",
      "1269: [D loss: 0.737356, acc: 0.535156]  [A loss: 0.961431, acc: 0.070312]\n",
      "1270: [D loss: 0.707452, acc: 0.503906]  [A loss: 0.816925, acc: 0.242188]\n",
      "1271: [D loss: 0.739699, acc: 0.468750]  [A loss: 0.991429, acc: 0.085938]\n",
      "1272: [D loss: 0.750124, acc: 0.476562]  [A loss: 0.868952, acc: 0.156250]\n",
      "1273: [D loss: 0.742626, acc: 0.476562]  [A loss: 0.950467, acc: 0.070312]\n",
      "1274: [D loss: 0.707012, acc: 0.480469]  [A loss: 0.869563, acc: 0.125000]\n",
      "1275: [D loss: 0.708758, acc: 0.503906]  [A loss: 0.938654, acc: 0.125000]\n",
      "1276: [D loss: 0.686328, acc: 0.562500]  [A loss: 0.817145, acc: 0.234375]\n",
      "1277: [D loss: 0.724683, acc: 0.515625]  [A loss: 0.984268, acc: 0.054688]\n",
      "1278: [D loss: 0.716227, acc: 0.460938]  [A loss: 0.916597, acc: 0.085938]\n",
      "1279: [D loss: 0.702435, acc: 0.531250]  [A loss: 0.873946, acc: 0.187500]\n",
      "1280: [D loss: 0.688877, acc: 0.542969]  [A loss: 0.871178, acc: 0.187500]\n",
      "1281: [D loss: 0.685920, acc: 0.546875]  [A loss: 0.974226, acc: 0.109375]\n",
      "1282: [D loss: 0.717302, acc: 0.519531]  [A loss: 1.012321, acc: 0.125000]\n",
      "1283: [D loss: 0.699483, acc: 0.554688]  [A loss: 1.113933, acc: 0.031250]\n",
      "1284: [D loss: 0.682087, acc: 0.574219]  [A loss: 0.793715, acc: 0.265625]\n",
      "1285: [D loss: 0.702869, acc: 0.539062]  [A loss: 1.112769, acc: 0.031250]\n",
      "1286: [D loss: 0.720706, acc: 0.507812]  [A loss: 0.763394, acc: 0.367188]\n",
      "1287: [D loss: 0.727412, acc: 0.507812]  [A loss: 1.088190, acc: 0.046875]\n",
      "1288: [D loss: 0.710021, acc: 0.492188]  [A loss: 0.690804, acc: 0.546875]\n",
      "1289: [D loss: 0.737361, acc: 0.484375]  [A loss: 1.066465, acc: 0.023438]\n",
      "1290: [D loss: 0.705051, acc: 0.519531]  [A loss: 0.770922, acc: 0.328125]\n",
      "1291: [D loss: 0.715278, acc: 0.507812]  [A loss: 0.997509, acc: 0.148438]\n",
      "1292: [D loss: 0.726129, acc: 0.488281]  [A loss: 0.768662, acc: 0.273438]\n",
      "1293: [D loss: 0.723327, acc: 0.523438]  [A loss: 0.899083, acc: 0.125000]\n",
      "1294: [D loss: 0.702298, acc: 0.523438]  [A loss: 0.812110, acc: 0.273438]\n",
      "1295: [D loss: 0.700290, acc: 0.531250]  [A loss: 0.962710, acc: 0.070312]\n",
      "1296: [D loss: 0.705918, acc: 0.519531]  [A loss: 0.825764, acc: 0.257812]\n",
      "1297: [D loss: 0.719725, acc: 0.480469]  [A loss: 0.959583, acc: 0.148438]\n",
      "1298: [D loss: 0.685179, acc: 0.558594]  [A loss: 0.773658, acc: 0.367188]\n",
      "1299: [D loss: 0.760117, acc: 0.500000]  [A loss: 1.142563, acc: 0.023438]\n",
      "1300: [D loss: 0.700409, acc: 0.558594]  [A loss: 0.650846, acc: 0.632812]\n",
      "1301: [D loss: 0.758458, acc: 0.511719]  [A loss: 1.116997, acc: 0.015625]\n",
      "1302: [D loss: 0.685278, acc: 0.582031]  [A loss: 0.740415, acc: 0.437500]\n",
      "1303: [D loss: 0.774776, acc: 0.507812]  [A loss: 1.014899, acc: 0.078125]\n",
      "1304: [D loss: 0.703368, acc: 0.460938]  [A loss: 1.164447, acc: 0.078125]\n",
      "1305: [D loss: 0.688793, acc: 0.578125]  [A loss: 0.849616, acc: 0.210938]\n",
      "1306: [D loss: 0.675151, acc: 0.550781]  [A loss: 0.964432, acc: 0.101562]\n",
      "1307: [D loss: 0.696068, acc: 0.503906]  [A loss: 0.793798, acc: 0.281250]\n",
      "1308: [D loss: 0.768399, acc: 0.464844]  [A loss: 1.071204, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1309: [D loss: 0.690976, acc: 0.535156]  [A loss: 0.710520, acc: 0.476562]\n",
      "1310: [D loss: 0.776198, acc: 0.496094]  [A loss: 1.109048, acc: 0.015625]\n",
      "1311: [D loss: 0.701544, acc: 0.515625]  [A loss: 0.733420, acc: 0.453125]\n",
      "1312: [D loss: 0.717344, acc: 0.515625]  [A loss: 0.972776, acc: 0.132812]\n",
      "1313: [D loss: 0.682904, acc: 0.589844]  [A loss: 0.728265, acc: 0.421875]\n",
      "1314: [D loss: 0.751656, acc: 0.472656]  [A loss: 0.928302, acc: 0.109375]\n",
      "1315: [D loss: 0.675135, acc: 0.554688]  [A loss: 0.779793, acc: 0.375000]\n",
      "1316: [D loss: 0.716948, acc: 0.566406]  [A loss: 0.927130, acc: 0.101562]\n",
      "1317: [D loss: 0.686481, acc: 0.574219]  [A loss: 0.786309, acc: 0.281250]\n",
      "1318: [D loss: 0.696043, acc: 0.566406]  [A loss: 0.892536, acc: 0.234375]\n",
      "1319: [D loss: 0.690746, acc: 0.554688]  [A loss: 0.906868, acc: 0.125000]\n",
      "1320: [D loss: 0.690957, acc: 0.539062]  [A loss: 0.762304, acc: 0.359375]\n",
      "1321: [D loss: 0.739972, acc: 0.476562]  [A loss: 0.993296, acc: 0.054688]\n",
      "1322: [D loss: 0.688380, acc: 0.558594]  [A loss: 0.769066, acc: 0.359375]\n",
      "1323: [D loss: 0.734007, acc: 0.507812]  [A loss: 1.017643, acc: 0.101562]\n",
      "1324: [D loss: 0.695283, acc: 0.531250]  [A loss: 0.782743, acc: 0.296875]\n",
      "1325: [D loss: 0.731732, acc: 0.500000]  [A loss: 0.966000, acc: 0.085938]\n",
      "1326: [D loss: 0.685547, acc: 0.535156]  [A loss: 0.784865, acc: 0.273438]\n",
      "1327: [D loss: 0.742800, acc: 0.539062]  [A loss: 0.926741, acc: 0.109375]\n",
      "1328: [D loss: 0.690554, acc: 0.546875]  [A loss: 0.803102, acc: 0.273438]\n",
      "1329: [D loss: 0.725509, acc: 0.527344]  [A loss: 0.964883, acc: 0.039062]\n",
      "1330: [D loss: 0.683730, acc: 0.550781]  [A loss: 0.827216, acc: 0.273438]\n",
      "1331: [D loss: 0.709174, acc: 0.542969]  [A loss: 0.941020, acc: 0.125000]\n",
      "1332: [D loss: 0.693439, acc: 0.542969]  [A loss: 0.750599, acc: 0.382812]\n",
      "1333: [D loss: 0.718906, acc: 0.511719]  [A loss: 0.953938, acc: 0.109375]\n",
      "1334: [D loss: 0.717212, acc: 0.480469]  [A loss: 0.770685, acc: 0.289062]\n",
      "1335: [D loss: 0.765014, acc: 0.492188]  [A loss: 1.076733, acc: 0.062500]\n",
      "1336: [D loss: 0.715410, acc: 0.519531]  [A loss: 1.042528, acc: 0.015625]\n",
      "1337: [D loss: 0.679961, acc: 0.597656]  [A loss: 0.718707, acc: 0.453125]\n",
      "1338: [D loss: 0.768480, acc: 0.503906]  [A loss: 1.115717, acc: 0.015625]\n",
      "1339: [D loss: 0.692344, acc: 0.539062]  [A loss: 0.738613, acc: 0.437500]\n",
      "1340: [D loss: 0.812377, acc: 0.496094]  [A loss: 1.105889, acc: 0.023438]\n",
      "1341: [D loss: 0.713796, acc: 0.539062]  [A loss: 0.789750, acc: 0.304688]\n",
      "1342: [D loss: 0.744674, acc: 0.503906]  [A loss: 0.968247, acc: 0.070312]\n",
      "1343: [D loss: 0.671900, acc: 0.558594]  [A loss: 0.721060, acc: 0.437500]\n",
      "1344: [D loss: 0.761563, acc: 0.488281]  [A loss: 1.112326, acc: 0.031250]\n",
      "1345: [D loss: 0.677457, acc: 0.562500]  [A loss: 0.828148, acc: 0.265625]\n",
      "1346: [D loss: 0.702148, acc: 0.503906]  [A loss: 0.942822, acc: 0.101562]\n",
      "1347: [D loss: 0.718397, acc: 0.500000]  [A loss: 0.881689, acc: 0.156250]\n",
      "1348: [D loss: 0.713324, acc: 0.507812]  [A loss: 0.775733, acc: 0.265625]\n",
      "1349: [D loss: 0.726354, acc: 0.496094]  [A loss: 0.880421, acc: 0.125000]\n",
      "1350: [D loss: 0.683577, acc: 0.593750]  [A loss: 0.822039, acc: 0.218750]\n",
      "1351: [D loss: 0.735261, acc: 0.484375]  [A loss: 0.845631, acc: 0.179688]\n",
      "1352: [D loss: 0.694124, acc: 0.550781]  [A loss: 0.838109, acc: 0.226562]\n",
      "1353: [D loss: 0.701754, acc: 0.515625]  [A loss: 0.957642, acc: 0.132812]\n",
      "1354: [D loss: 0.703289, acc: 0.507812]  [A loss: 0.756031, acc: 0.382812]\n",
      "1355: [D loss: 0.725125, acc: 0.507812]  [A loss: 0.944009, acc: 0.078125]\n",
      "1356: [D loss: 0.686515, acc: 0.582031]  [A loss: 0.814578, acc: 0.226562]\n",
      "1357: [D loss: 0.709049, acc: 0.496094]  [A loss: 1.006676, acc: 0.093750]\n",
      "1358: [D loss: 0.689458, acc: 0.550781]  [A loss: 0.750485, acc: 0.367188]\n",
      "1359: [D loss: 0.721012, acc: 0.515625]  [A loss: 0.966048, acc: 0.070312]\n",
      "1360: [D loss: 0.684605, acc: 0.542969]  [A loss: 0.744413, acc: 0.414062]\n",
      "1361: [D loss: 0.740233, acc: 0.515625]  [A loss: 1.011313, acc: 0.093750]\n",
      "1362: [D loss: 0.708166, acc: 0.507812]  [A loss: 0.819564, acc: 0.265625]\n",
      "1363: [D loss: 0.706310, acc: 0.523438]  [A loss: 0.966478, acc: 0.109375]\n",
      "1364: [D loss: 0.683544, acc: 0.550781]  [A loss: 0.803963, acc: 0.257812]\n",
      "1365: [D loss: 0.724496, acc: 0.492188]  [A loss: 1.020233, acc: 0.117188]\n",
      "1366: [D loss: 0.703588, acc: 0.523438]  [A loss: 0.681729, acc: 0.554688]\n",
      "1367: [D loss: 0.780244, acc: 0.488281]  [A loss: 1.117066, acc: 0.046875]\n",
      "1368: [D loss: 0.699368, acc: 0.531250]  [A loss: 0.695427, acc: 0.500000]\n",
      "1369: [D loss: 0.764182, acc: 0.484375]  [A loss: 1.004585, acc: 0.085938]\n",
      "1370: [D loss: 0.719798, acc: 0.484375]  [A loss: 0.792349, acc: 0.289062]\n",
      "1371: [D loss: 0.710297, acc: 0.558594]  [A loss: 0.890574, acc: 0.156250]\n",
      "1372: [D loss: 0.694652, acc: 0.546875]  [A loss: 0.898899, acc: 0.203125]\n",
      "1373: [D loss: 0.713304, acc: 0.507812]  [A loss: 0.929148, acc: 0.109375]\n",
      "1374: [D loss: 0.697308, acc: 0.535156]  [A loss: 0.804319, acc: 0.289062]\n",
      "1375: [D loss: 0.698345, acc: 0.523438]  [A loss: 0.819494, acc: 0.281250]\n",
      "1376: [D loss: 0.703077, acc: 0.523438]  [A loss: 0.947469, acc: 0.109375]\n",
      "1377: [D loss: 0.687670, acc: 0.539062]  [A loss: 0.770517, acc: 0.382812]\n",
      "1378: [D loss: 0.718204, acc: 0.531250]  [A loss: 1.042356, acc: 0.039062]\n",
      "1379: [D loss: 0.697356, acc: 0.527344]  [A loss: 0.753565, acc: 0.351562]\n",
      "1380: [D loss: 0.733356, acc: 0.515625]  [A loss: 1.044760, acc: 0.078125]\n",
      "1381: [D loss: 0.728787, acc: 0.484375]  [A loss: 0.909143, acc: 0.093750]\n",
      "1382: [D loss: 0.689791, acc: 0.554688]  [A loss: 0.870276, acc: 0.203125]\n",
      "1383: [D loss: 0.702930, acc: 0.500000]  [A loss: 0.829020, acc: 0.281250]\n",
      "1384: [D loss: 0.710662, acc: 0.500000]  [A loss: 2.144680, acc: 0.000000]\n",
      "1385: [D loss: 0.719214, acc: 0.507812]  [A loss: 0.938530, acc: 0.250000]\n",
      "1386: [D loss: 0.690721, acc: 0.546875]  [A loss: 1.177215, acc: 0.085938]\n",
      "1387: [D loss: 0.661155, acc: 0.582031]  [A loss: 0.677458, acc: 0.585938]\n",
      "1388: [D loss: 0.765164, acc: 0.515625]  [A loss: 1.125054, acc: 0.031250]\n",
      "1389: [D loss: 0.694390, acc: 0.542969]  [A loss: 0.711592, acc: 0.460938]\n",
      "1390: [D loss: 0.708761, acc: 0.523438]  [A loss: 1.003724, acc: 0.093750]\n",
      "1391: [D loss: 0.698567, acc: 0.527344]  [A loss: 0.794277, acc: 0.343750]\n",
      "1392: [D loss: 0.717339, acc: 0.476562]  [A loss: 0.866071, acc: 0.179688]\n",
      "1393: [D loss: 0.711262, acc: 0.500000]  [A loss: 0.779231, acc: 0.367188]\n",
      "1394: [D loss: 0.685947, acc: 0.531250]  [A loss: 0.900860, acc: 0.148438]\n",
      "1395: [D loss: 0.693950, acc: 0.570312]  [A loss: 0.849777, acc: 0.218750]\n",
      "1396: [D loss: 0.687791, acc: 0.542969]  [A loss: 0.756091, acc: 0.382812]\n",
      "1397: [D loss: 0.694158, acc: 0.535156]  [A loss: 0.919157, acc: 0.148438]\n",
      "1398: [D loss: 0.699018, acc: 0.515625]  [A loss: 0.807600, acc: 0.242188]\n",
      "1399: [D loss: 0.734811, acc: 0.460938]  [A loss: 0.944959, acc: 0.125000]\n",
      "1400: [D loss: 0.675837, acc: 0.605469]  [A loss: 0.718074, acc: 0.460938]\n",
      "1401: [D loss: 0.706791, acc: 0.503906]  [A loss: 0.916853, acc: 0.117188]\n",
      "1402: [D loss: 0.680318, acc: 0.527344]  [A loss: 0.781559, acc: 0.335938]\n",
      "1403: [D loss: 0.723074, acc: 0.511719]  [A loss: 0.940848, acc: 0.117188]\n",
      "1404: [D loss: 0.678738, acc: 0.566406]  [A loss: 0.768161, acc: 0.328125]\n",
      "1405: [D loss: 0.722749, acc: 0.500000]  [A loss: 0.877087, acc: 0.234375]\n",
      "1406: [D loss: 0.691846, acc: 0.542969]  [A loss: 0.882638, acc: 0.210938]\n",
      "1407: [D loss: 0.718360, acc: 0.515625]  [A loss: 1.002376, acc: 0.078125]\n",
      "1408: [D loss: 0.681276, acc: 0.566406]  [A loss: 0.746989, acc: 0.406250]\n",
      "1409: [D loss: 0.695986, acc: 0.503906]  [A loss: 0.949078, acc: 0.132812]\n",
      "1410: [D loss: 0.727375, acc: 0.496094]  [A loss: 0.828326, acc: 0.203125]\n",
      "1411: [D loss: 0.709051, acc: 0.507812]  [A loss: 0.883192, acc: 0.187500]\n",
      "1412: [D loss: 0.695190, acc: 0.546875]  [A loss: 0.861617, acc: 0.234375]\n",
      "1413: [D loss: 0.714268, acc: 0.507812]  [A loss: 0.867991, acc: 0.218750]\n",
      "1414: [D loss: 0.702510, acc: 0.507812]  [A loss: 0.799112, acc: 0.296875]\n",
      "1415: [D loss: 0.722922, acc: 0.527344]  [A loss: 0.994509, acc: 0.046875]\n",
      "1416: [D loss: 0.686334, acc: 0.558594]  [A loss: 0.710768, acc: 0.453125]\n",
      "1417: [D loss: 0.728499, acc: 0.500000]  [A loss: 1.084900, acc: 0.031250]\n",
      "1418: [D loss: 0.691179, acc: 0.515625]  [A loss: 0.726699, acc: 0.429688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419: [D loss: 0.716612, acc: 0.500000]  [A loss: 0.957986, acc: 0.093750]\n",
      "1420: [D loss: 0.700458, acc: 0.535156]  [A loss: 0.807864, acc: 0.281250]\n",
      "1421: [D loss: 0.713091, acc: 0.531250]  [A loss: 0.896030, acc: 0.148438]\n",
      "1422: [D loss: 0.695882, acc: 0.546875]  [A loss: 0.843516, acc: 0.210938]\n",
      "1423: [D loss: 0.686195, acc: 0.582031]  [A loss: 0.880019, acc: 0.203125]\n",
      "1424: [D loss: 0.702773, acc: 0.511719]  [A loss: 0.839274, acc: 0.218750]\n",
      "1425: [D loss: 0.712221, acc: 0.531250]  [A loss: 0.954577, acc: 0.125000]\n",
      "1426: [D loss: 0.694683, acc: 0.546875]  [A loss: 0.914452, acc: 0.125000]\n",
      "1427: [D loss: 0.706499, acc: 0.515625]  [A loss: 0.946772, acc: 0.132812]\n",
      "1428: [D loss: 0.681178, acc: 0.554688]  [A loss: 0.852500, acc: 0.164062]\n",
      "1429: [D loss: 0.726464, acc: 0.546875]  [A loss: 0.970592, acc: 0.039062]\n",
      "1430: [D loss: 0.695485, acc: 0.503906]  [A loss: 0.764280, acc: 0.335938]\n",
      "1431: [D loss: 0.760480, acc: 0.492188]  [A loss: 1.056919, acc: 0.031250]\n",
      "1432: [D loss: 0.717251, acc: 0.488281]  [A loss: 0.823014, acc: 0.171875]\n",
      "1433: [D loss: 0.700792, acc: 0.519531]  [A loss: 1.000528, acc: 0.046875]\n",
      "1434: [D loss: 0.703673, acc: 0.519531]  [A loss: 0.870185, acc: 0.218750]\n",
      "1435: [D loss: 0.692493, acc: 0.535156]  [A loss: 1.030651, acc: 0.062500]\n",
      "1436: [D loss: 0.715797, acc: 0.523438]  [A loss: 0.864202, acc: 0.148438]\n",
      "1437: [D loss: 0.729433, acc: 0.515625]  [A loss: 0.948393, acc: 0.078125]\n",
      "1438: [D loss: 0.703235, acc: 0.503906]  [A loss: 0.876493, acc: 0.234375]\n",
      "1439: [D loss: 0.702109, acc: 0.550781]  [A loss: 0.868669, acc: 0.179688]\n",
      "1440: [D loss: 0.752303, acc: 0.492188]  [A loss: 1.251640, acc: 0.015625]\n",
      "1441: [D loss: 0.726384, acc: 0.496094]  [A loss: 0.732534, acc: 0.460938]\n",
      "1442: [D loss: 1.023690, acc: 0.503906]  [A loss: 0.741478, acc: 0.406250]\n",
      "1443: [D loss: 0.818629, acc: 0.496094]  [A loss: 1.123313, acc: 0.000000]\n",
      "1444: [D loss: 0.700080, acc: 0.519531]  [A loss: 0.763876, acc: 0.320312]\n",
      "1445: [D loss: 0.743817, acc: 0.496094]  [A loss: 1.022436, acc: 0.031250]\n",
      "1446: [D loss: 0.713475, acc: 0.464844]  [A loss: 0.772483, acc: 0.265625]\n",
      "1447: [D loss: 0.741496, acc: 0.468750]  [A loss: 0.849219, acc: 0.148438]\n",
      "1448: [D loss: 0.717672, acc: 0.492188]  [A loss: 0.812805, acc: 0.226562]\n",
      "1449: [D loss: 0.726871, acc: 0.496094]  [A loss: 0.828347, acc: 0.140625]\n",
      "1450: [D loss: 0.722448, acc: 0.445312]  [A loss: 0.803849, acc: 0.218750]\n",
      "1451: [D loss: 0.697499, acc: 0.511719]  [A loss: 0.852046, acc: 0.109375]\n",
      "1452: [D loss: 0.693898, acc: 0.527344]  [A loss: 0.835899, acc: 0.171875]\n",
      "1453: [D loss: 0.701551, acc: 0.519531]  [A loss: 0.831922, acc: 0.179688]\n",
      "1454: [D loss: 0.704519, acc: 0.519531]  [A loss: 0.846610, acc: 0.109375]\n",
      "1455: [D loss: 0.712984, acc: 0.488281]  [A loss: 0.816772, acc: 0.242188]\n",
      "1456: [D loss: 0.691490, acc: 0.550781]  [A loss: 0.929420, acc: 0.093750]\n",
      "1457: [D loss: 0.698857, acc: 0.511719]  [A loss: 0.787806, acc: 0.281250]\n",
      "1458: [D loss: 0.738383, acc: 0.500000]  [A loss: 1.066945, acc: 0.046875]\n",
      "1459: [D loss: 0.707018, acc: 0.496094]  [A loss: 0.711298, acc: 0.515625]\n",
      "1460: [D loss: 0.765784, acc: 0.507812]  [A loss: 1.031678, acc: 0.015625]\n",
      "1461: [D loss: 0.679457, acc: 0.574219]  [A loss: 0.732901, acc: 0.375000]\n",
      "1462: [D loss: 0.742300, acc: 0.511719]  [A loss: 1.003703, acc: 0.054688]\n",
      "1463: [D loss: 0.690208, acc: 0.539062]  [A loss: 0.736205, acc: 0.375000]\n",
      "1464: [D loss: 0.741643, acc: 0.464844]  [A loss: 0.913951, acc: 0.070312]\n",
      "1465: [D loss: 0.688525, acc: 0.550781]  [A loss: 0.766787, acc: 0.382812]\n",
      "1466: [D loss: 0.709062, acc: 0.496094]  [A loss: 0.892839, acc: 0.156250]\n",
      "1467: [D loss: 0.702202, acc: 0.527344]  [A loss: 0.771294, acc: 0.320312]\n",
      "1468: [D loss: 0.713929, acc: 0.484375]  [A loss: 0.868060, acc: 0.187500]\n",
      "1469: [D loss: 0.706087, acc: 0.519531]  [A loss: 0.814641, acc: 0.195312]\n",
      "1470: [D loss: 0.693247, acc: 0.578125]  [A loss: 0.784949, acc: 0.281250]\n",
      "1471: [D loss: 0.726446, acc: 0.546875]  [A loss: 0.986354, acc: 0.031250]\n",
      "1472: [D loss: 0.696828, acc: 0.519531]  [A loss: 0.778456, acc: 0.289062]\n",
      "1473: [D loss: 0.722876, acc: 0.515625]  [A loss: 1.022190, acc: 0.007812]\n",
      "1474: [D loss: 0.700543, acc: 0.519531]  [A loss: 0.829877, acc: 0.187500]\n",
      "1475: [D loss: 0.726568, acc: 0.496094]  [A loss: 0.959431, acc: 0.031250]\n",
      "1476: [D loss: 0.694834, acc: 0.531250]  [A loss: 0.714700, acc: 0.453125]\n",
      "1477: [D loss: 0.728256, acc: 0.515625]  [A loss: 0.997125, acc: 0.078125]\n",
      "1478: [D loss: 0.709945, acc: 0.484375]  [A loss: 0.695303, acc: 0.515625]\n",
      "1479: [D loss: 0.767256, acc: 0.503906]  [A loss: 1.002690, acc: 0.085938]\n",
      "1480: [D loss: 0.681614, acc: 0.562500]  [A loss: 0.797321, acc: 0.296875]\n",
      "1481: [D loss: 0.728657, acc: 0.523438]  [A loss: 0.926629, acc: 0.101562]\n",
      "1482: [D loss: 0.677252, acc: 0.558594]  [A loss: 0.729410, acc: 0.445312]\n",
      "1483: [D loss: 0.736445, acc: 0.503906]  [A loss: 0.965662, acc: 0.078125]\n",
      "1484: [D loss: 0.689572, acc: 0.546875]  [A loss: 0.790400, acc: 0.242188]\n",
      "1485: [D loss: 0.704422, acc: 0.507812]  [A loss: 0.908266, acc: 0.117188]\n",
      "1486: [D loss: 0.692823, acc: 0.558594]  [A loss: 0.813313, acc: 0.265625]\n",
      "1487: [D loss: 0.698040, acc: 0.519531]  [A loss: 0.879129, acc: 0.171875]\n",
      "1488: [D loss: 0.685845, acc: 0.542969]  [A loss: 0.831695, acc: 0.250000]\n",
      "1489: [D loss: 0.714789, acc: 0.539062]  [A loss: 1.362000, acc: 0.007812]\n",
      "1490: [D loss: 0.693643, acc: 0.546875]  [A loss: 0.700133, acc: 0.507812]\n",
      "1491: [D loss: 0.743288, acc: 0.515625]  [A loss: 1.027021, acc: 0.046875]\n",
      "1492: [D loss: 0.677697, acc: 0.589844]  [A loss: 0.716926, acc: 0.437500]\n",
      "1493: [D loss: 0.742375, acc: 0.492188]  [A loss: 0.897429, acc: 0.070312]\n",
      "1494: [D loss: 0.682906, acc: 0.566406]  [A loss: 0.746151, acc: 0.390625]\n",
      "1495: [D loss: 0.715462, acc: 0.507812]  [A loss: 0.930733, acc: 0.101562]\n",
      "1496: [D loss: 0.723799, acc: 0.453125]  [A loss: 0.809691, acc: 0.257812]\n",
      "1497: [D loss: 0.685919, acc: 0.527344]  [A loss: 0.824428, acc: 0.226562]\n",
      "1498: [D loss: 0.685075, acc: 0.570312]  [A loss: 0.802439, acc: 0.296875]\n",
      "1499: [D loss: 0.714156, acc: 0.546875]  [A loss: 0.831692, acc: 0.242188]\n",
      "1500: [D loss: 0.710483, acc: 0.503906]  [A loss: 0.881094, acc: 0.148438]\n",
      "1501: [D loss: 0.688961, acc: 0.574219]  [A loss: 0.893856, acc: 0.187500]\n",
      "1502: [D loss: 0.687190, acc: 0.558594]  [A loss: 0.763196, acc: 0.320312]\n",
      "1503: [D loss: 0.710844, acc: 0.507812]  [A loss: 1.108478, acc: 0.054688]\n",
      "1504: [D loss: 0.662723, acc: 0.570312]  [A loss: 0.769524, acc: 0.328125]\n",
      "1505: [D loss: 0.776864, acc: 0.480469]  [A loss: 1.016445, acc: 0.054688]\n",
      "1506: [D loss: 0.678752, acc: 0.562500]  [A loss: 0.840351, acc: 0.234375]\n",
      "1507: [D loss: 0.724421, acc: 0.476562]  [A loss: 0.909299, acc: 0.132812]\n",
      "1508: [D loss: 0.689664, acc: 0.550781]  [A loss: 0.785888, acc: 0.328125]\n",
      "1509: [D loss: 0.708965, acc: 0.515625]  [A loss: 1.064308, acc: 0.015625]\n",
      "1510: [D loss: 0.699325, acc: 0.523438]  [A loss: 0.818350, acc: 0.312500]\n",
      "1511: [D loss: 0.768010, acc: 0.472656]  [A loss: 1.232571, acc: 0.000000]\n",
      "1512: [D loss: 0.684825, acc: 0.554688]  [A loss: 0.910355, acc: 0.132812]\n",
      "1513: [D loss: 0.716555, acc: 0.507812]  [A loss: 0.989601, acc: 0.054688]\n",
      "1514: [D loss: 0.694591, acc: 0.503906]  [A loss: 0.826711, acc: 0.273438]\n",
      "1515: [D loss: 0.685671, acc: 0.546875]  [A loss: 0.949290, acc: 0.085938]\n",
      "1516: [D loss: 0.687303, acc: 0.562500]  [A loss: 0.894646, acc: 0.203125]\n",
      "1517: [D loss: 0.701054, acc: 0.542969]  [A loss: 1.020262, acc: 0.039062]\n",
      "1518: [D loss: 0.708595, acc: 0.515625]  [A loss: 0.828259, acc: 0.265625]\n",
      "1519: [D loss: 0.705483, acc: 0.515625]  [A loss: 0.899176, acc: 0.218750]\n",
      "1520: [D loss: 0.684840, acc: 0.578125]  [A loss: 0.827532, acc: 0.273438]\n",
      "1521: [D loss: 0.720671, acc: 0.503906]  [A loss: 1.000633, acc: 0.117188]\n",
      "1522: [D loss: 0.722660, acc: 0.460938]  [A loss: 0.972982, acc: 0.140625]\n",
      "1523: [D loss: 0.682182, acc: 0.539062]  [A loss: 0.838893, acc: 0.195312]\n",
      "1524: [D loss: 0.747566, acc: 0.523438]  [A loss: 1.073479, acc: 0.015625]\n",
      "1525: [D loss: 0.683753, acc: 0.582031]  [A loss: 0.689991, acc: 0.562500]\n",
      "1526: [D loss: 0.747727, acc: 0.500000]  [A loss: 1.028303, acc: 0.046875]\n",
      "1527: [D loss: 0.693398, acc: 0.527344]  [A loss: 0.788859, acc: 0.320312]\n",
      "1528: [D loss: 0.776649, acc: 0.453125]  [A loss: 1.025054, acc: 0.070312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529: [D loss: 0.699744, acc: 0.523438]  [A loss: 0.816606, acc: 0.257812]\n",
      "1530: [D loss: 0.718599, acc: 0.515625]  [A loss: 0.943535, acc: 0.085938]\n",
      "1531: [D loss: 0.685029, acc: 0.550781]  [A loss: 0.760915, acc: 0.429688]\n",
      "1532: [D loss: 0.708587, acc: 0.562500]  [A loss: 0.912861, acc: 0.156250]\n",
      "1533: [D loss: 0.688276, acc: 0.570312]  [A loss: 0.756723, acc: 0.414062]\n",
      "1534: [D loss: 0.732977, acc: 0.519531]  [A loss: 0.968057, acc: 0.085938]\n",
      "1535: [D loss: 0.708628, acc: 0.500000]  [A loss: 0.787682, acc: 0.335938]\n",
      "1536: [D loss: 0.714440, acc: 0.535156]  [A loss: 0.921246, acc: 0.164062]\n",
      "1537: [D loss: 0.675765, acc: 0.574219]  [A loss: 0.778306, acc: 0.312500]\n",
      "1538: [D loss: 0.730222, acc: 0.511719]  [A loss: 0.884029, acc: 0.148438]\n",
      "1539: [D loss: 0.697454, acc: 0.531250]  [A loss: 0.855252, acc: 0.148438]\n",
      "1540: [D loss: 0.705949, acc: 0.531250]  [A loss: 0.864674, acc: 0.132812]\n",
      "1541: [D loss: 0.706623, acc: 0.507812]  [A loss: 0.939075, acc: 0.054688]\n",
      "1542: [D loss: 0.679883, acc: 0.570312]  [A loss: 0.894983, acc: 0.203125]\n",
      "1543: [D loss: 0.711792, acc: 0.488281]  [A loss: 0.930230, acc: 0.132812]\n",
      "1544: [D loss: 0.722278, acc: 0.484375]  [A loss: 0.855108, acc: 0.164062]\n",
      "1545: [D loss: 0.705074, acc: 0.488281]  [A loss: 0.991052, acc: 0.046875]\n",
      "1546: [D loss: 0.690988, acc: 0.550781]  [A loss: 0.991106, acc: 0.117188]\n",
      "1547: [D loss: 0.705866, acc: 0.546875]  [A loss: 0.992720, acc: 0.101562]\n",
      "1548: [D loss: 0.695847, acc: 0.566406]  [A loss: 0.799695, acc: 0.273438]\n",
      "1549: [D loss: 0.701447, acc: 0.542969]  [A loss: 1.040951, acc: 0.093750]\n",
      "1550: [D loss: 0.667034, acc: 0.601562]  [A loss: 0.720093, acc: 0.453125]\n",
      "1551: [D loss: 0.758679, acc: 0.507812]  [A loss: 1.053101, acc: 0.015625]\n",
      "1552: [D loss: 0.697682, acc: 0.535156]  [A loss: 0.840840, acc: 0.195312]\n",
      "1553: [D loss: 0.727381, acc: 0.492188]  [A loss: 0.975674, acc: 0.140625]\n",
      "1554: [D loss: 0.684353, acc: 0.574219]  [A loss: 0.815951, acc: 0.226562]\n",
      "1555: [D loss: 0.718415, acc: 0.527344]  [A loss: 0.924147, acc: 0.078125]\n",
      "1556: [D loss: 0.675877, acc: 0.546875]  [A loss: 0.885906, acc: 0.132812]\n",
      "1557: [D loss: 0.705984, acc: 0.535156]  [A loss: 0.997403, acc: 0.101562]\n",
      "1558: [D loss: 0.700221, acc: 0.523438]  [A loss: 0.868605, acc: 0.195312]\n",
      "1559: [D loss: 0.701249, acc: 0.578125]  [A loss: 0.948095, acc: 0.125000]\n",
      "1560: [D loss: 0.673132, acc: 0.558594]  [A loss: 0.815895, acc: 0.320312]\n",
      "1561: [D loss: 0.704878, acc: 0.527344]  [A loss: 1.079152, acc: 0.054688]\n",
      "1562: [D loss: 0.713741, acc: 0.531250]  [A loss: 3.029339, acc: 0.000000]\n",
      "1563: [D loss: 0.617673, acc: 0.652344]  [A loss: 0.417923, acc: 0.906250]\n",
      "1564: [D loss: 1.123324, acc: 0.496094]  [A loss: 1.102172, acc: 0.054688]\n",
      "1565: [D loss: 0.659277, acc: 0.628906]  [A loss: 0.777380, acc: 0.375000]\n",
      "1566: [D loss: 0.765315, acc: 0.503906]  [A loss: 1.170825, acc: 0.023438]\n",
      "1567: [D loss: 0.697995, acc: 0.582031]  [A loss: 0.894633, acc: 0.187500]\n",
      "1568: [D loss: 0.712188, acc: 0.507812]  [A loss: 0.887960, acc: 0.148438]\n",
      "1569: [D loss: 0.672586, acc: 0.562500]  [A loss: 0.745766, acc: 0.398438]\n",
      "1570: [D loss: 0.718085, acc: 0.519531]  [A loss: 0.878818, acc: 0.164062]\n",
      "1571: [D loss: 0.680352, acc: 0.554688]  [A loss: 0.745185, acc: 0.421875]\n",
      "1572: [D loss: 0.699463, acc: 0.550781]  [A loss: 0.784047, acc: 0.375000]\n",
      "1573: [D loss: 0.728193, acc: 0.515625]  [A loss: 0.828400, acc: 0.234375]\n",
      "1574: [D loss: 0.696469, acc: 0.535156]  [A loss: 0.794501, acc: 0.320312]\n",
      "1575: [D loss: 0.689749, acc: 0.562500]  [A loss: 0.860239, acc: 0.164062]\n",
      "1576: [D loss: 0.679543, acc: 0.570312]  [A loss: 0.803618, acc: 0.328125]\n",
      "1577: [D loss: 0.685641, acc: 0.546875]  [A loss: 0.782904, acc: 0.390625]\n",
      "1578: [D loss: 0.727133, acc: 0.500000]  [A loss: 0.811141, acc: 0.250000]\n",
      "1579: [D loss: 0.695240, acc: 0.566406]  [A loss: 0.865207, acc: 0.179688]\n",
      "1580: [D loss: 0.703358, acc: 0.539062]  [A loss: 0.886542, acc: 0.148438]\n",
      "1581: [D loss: 0.689379, acc: 0.570312]  [A loss: 0.852609, acc: 0.234375]\n",
      "1582: [D loss: 0.698456, acc: 0.558594]  [A loss: 0.849757, acc: 0.195312]\n",
      "1583: [D loss: 0.693087, acc: 0.585938]  [A loss: 0.866609, acc: 0.203125]\n",
      "1584: [D loss: 0.704348, acc: 0.511719]  [A loss: 0.910469, acc: 0.132812]\n",
      "1585: [D loss: 0.683555, acc: 0.562500]  [A loss: 0.760580, acc: 0.375000]\n",
      "1586: [D loss: 0.726260, acc: 0.507812]  [A loss: 0.903280, acc: 0.125000]\n",
      "1587: [D loss: 0.698569, acc: 0.527344]  [A loss: 0.831074, acc: 0.250000]\n",
      "1588: [D loss: 0.706812, acc: 0.527344]  [A loss: 0.807469, acc: 0.265625]\n",
      "1589: [D loss: 0.698238, acc: 0.546875]  [A loss: 0.942439, acc: 0.148438]\n",
      "1590: [D loss: 0.694870, acc: 0.562500]  [A loss: 0.872058, acc: 0.296875]\n",
      "1591: [D loss: 0.799442, acc: 0.492188]  [A loss: 0.873035, acc: 0.171875]\n",
      "1592: [D loss: 0.711913, acc: 0.523438]  [A loss: 1.044734, acc: 0.093750]\n",
      "1593: [D loss: 0.718028, acc: 0.492188]  [A loss: 0.892554, acc: 0.234375]\n",
      "1594: [D loss: 0.713628, acc: 0.515625]  [A loss: 0.979669, acc: 0.070312]\n",
      "1595: [D loss: 0.687983, acc: 0.550781]  [A loss: 0.757296, acc: 0.390625]\n",
      "1596: [D loss: 0.768283, acc: 0.500000]  [A loss: 1.181912, acc: 0.031250]\n",
      "1597: [D loss: 0.743780, acc: 0.472656]  [A loss: 0.809557, acc: 0.242188]\n",
      "1598: [D loss: 0.718678, acc: 0.515625]  [A loss: 0.927108, acc: 0.093750]\n",
      "1599: [D loss: 0.690013, acc: 0.597656]  [A loss: 0.808007, acc: 0.320312]\n",
      "1600: [D loss: 0.710171, acc: 0.542969]  [A loss: 0.917119, acc: 0.132812]\n",
      "1601: [D loss: 0.688453, acc: 0.585938]  [A loss: 0.789056, acc: 0.343750]\n",
      "1602: [D loss: 0.712318, acc: 0.531250]  [A loss: 0.979336, acc: 0.093750]\n",
      "1603: [D loss: 0.694536, acc: 0.523438]  [A loss: 0.774277, acc: 0.351562]\n",
      "1604: [D loss: 0.720005, acc: 0.519531]  [A loss: 0.925741, acc: 0.101562]\n",
      "1605: [D loss: 0.704947, acc: 0.531250]  [A loss: 0.817957, acc: 0.296875]\n",
      "1606: [D loss: 0.698324, acc: 0.515625]  [A loss: 0.958849, acc: 0.125000]\n",
      "1607: [D loss: 0.683474, acc: 0.554688]  [A loss: 0.821061, acc: 0.304688]\n",
      "1608: [D loss: 0.698725, acc: 0.515625]  [A loss: 1.037854, acc: 0.093750]\n",
      "1609: [D loss: 0.685172, acc: 0.554688]  [A loss: 0.900422, acc: 0.226562]\n",
      "1610: [D loss: 0.695611, acc: 0.550781]  [A loss: 1.024173, acc: 0.031250]\n",
      "1611: [D loss: 0.710148, acc: 0.523438]  [A loss: 0.905159, acc: 0.156250]\n",
      "1612: [D loss: 0.693601, acc: 0.562500]  [A loss: 0.773386, acc: 0.382812]\n",
      "1613: [D loss: 0.716002, acc: 0.511719]  [A loss: 1.012747, acc: 0.046875]\n",
      "1614: [D loss: 0.700994, acc: 0.500000]  [A loss: 0.763535, acc: 0.351562]\n",
      "1615: [D loss: 0.725857, acc: 0.507812]  [A loss: 1.050727, acc: 0.046875]\n",
      "1616: [D loss: 0.687720, acc: 0.558594]  [A loss: 0.732846, acc: 0.390625]\n",
      "1617: [D loss: 0.716225, acc: 0.527344]  [A loss: 0.903148, acc: 0.171875]\n",
      "1618: [D loss: 0.699187, acc: 0.503906]  [A loss: 0.874099, acc: 0.156250]\n",
      "1619: [D loss: 0.713142, acc: 0.535156]  [A loss: 0.962227, acc: 0.140625]\n",
      "1620: [D loss: 0.695012, acc: 0.535156]  [A loss: 0.794069, acc: 0.406250]\n",
      "1621: [D loss: 0.724531, acc: 0.500000]  [A loss: 0.955029, acc: 0.125000]\n",
      "1622: [D loss: 0.674454, acc: 0.578125]  [A loss: 0.834040, acc: 0.218750]\n",
      "1623: [D loss: 0.705299, acc: 0.527344]  [A loss: 0.983309, acc: 0.078125]\n",
      "1624: [D loss: 0.689889, acc: 0.542969]  [A loss: 0.841028, acc: 0.257812]\n",
      "1625: [D loss: 0.713971, acc: 0.535156]  [A loss: 0.921762, acc: 0.156250]\n",
      "1626: [D loss: 0.701549, acc: 0.562500]  [A loss: 0.835398, acc: 0.242188]\n",
      "1627: [D loss: 0.733713, acc: 0.496094]  [A loss: 1.018802, acc: 0.078125]\n",
      "1628: [D loss: 0.705224, acc: 0.488281]  [A loss: 0.815808, acc: 0.335938]\n",
      "1629: [D loss: 0.705654, acc: 0.539062]  [A loss: 1.132439, acc: 0.023438]\n",
      "1630: [D loss: 0.701783, acc: 0.523438]  [A loss: 0.827072, acc: 0.265625]\n",
      "1631: [D loss: 0.686292, acc: 0.558594]  [A loss: 1.002838, acc: 0.109375]\n",
      "1632: [D loss: 0.687746, acc: 0.582031]  [A loss: 0.829075, acc: 0.250000]\n",
      "1633: [D loss: 0.739282, acc: 0.519531]  [A loss: 1.056837, acc: 0.046875]\n",
      "1634: [D loss: 0.678381, acc: 0.523438]  [A loss: 1.033473, acc: 0.171875]\n",
      "1635: [D loss: 0.685456, acc: 0.562500]  [A loss: 0.891979, acc: 0.140625]\n",
      "1636: [D loss: 0.715074, acc: 0.503906]  [A loss: 0.975024, acc: 0.132812]\n",
      "1637: [D loss: 0.677167, acc: 0.542969]  [A loss: 0.747502, acc: 0.390625]\n",
      "1638: [D loss: 0.757562, acc: 0.523438]  [A loss: 1.077961, acc: 0.078125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639: [D loss: 0.687302, acc: 0.523438]  [A loss: 0.884028, acc: 0.257812]\n",
      "1640: [D loss: 0.744604, acc: 0.488281]  [A loss: 1.079618, acc: 0.054688]\n",
      "1641: [D loss: 0.696811, acc: 0.523438]  [A loss: 0.922093, acc: 0.171875]\n",
      "1642: [D loss: 0.697684, acc: 0.515625]  [A loss: 0.837895, acc: 0.226562]\n",
      "1643: [D loss: 0.696625, acc: 0.539062]  [A loss: 0.961673, acc: 0.085938]\n",
      "1644: [D loss: 0.722324, acc: 0.437500]  [A loss: 0.872648, acc: 0.218750]\n",
      "1645: [D loss: 0.700663, acc: 0.542969]  [A loss: 0.935351, acc: 0.117188]\n",
      "1646: [D loss: 0.691895, acc: 0.550781]  [A loss: 0.917342, acc: 0.109375]\n",
      "1647: [D loss: 0.687965, acc: 0.558594]  [A loss: 0.899354, acc: 0.156250]\n",
      "1648: [D loss: 0.662428, acc: 0.617188]  [A loss: 0.801090, acc: 0.320312]\n",
      "1649: [D loss: 0.741364, acc: 0.492188]  [A loss: 1.092568, acc: 0.031250]\n",
      "1650: [D loss: 0.715363, acc: 0.496094]  [A loss: 0.720811, acc: 0.468750]\n",
      "1651: [D loss: 0.724034, acc: 0.480469]  [A loss: 0.994215, acc: 0.085938]\n",
      "1652: [D loss: 0.683766, acc: 0.546875]  [A loss: 0.835916, acc: 0.281250]\n",
      "1653: [D loss: 0.691667, acc: 0.539062]  [A loss: 0.965165, acc: 0.132812]\n",
      "1654: [D loss: 0.708689, acc: 0.550781]  [A loss: 0.848342, acc: 0.320312]\n",
      "1655: [D loss: 0.714451, acc: 0.523438]  [A loss: 0.969533, acc: 0.078125]\n",
      "1656: [D loss: 0.704253, acc: 0.539062]  [A loss: 0.892837, acc: 0.242188]\n",
      "1657: [D loss: 0.736674, acc: 0.511719]  [A loss: 0.958141, acc: 0.054688]\n",
      "1658: [D loss: 0.696501, acc: 0.507812]  [A loss: 0.835524, acc: 0.156250]\n",
      "1659: [D loss: 0.717525, acc: 0.515625]  [A loss: 0.998230, acc: 0.062500]\n",
      "1660: [D loss: 0.700528, acc: 0.527344]  [A loss: 0.844128, acc: 0.210938]\n",
      "1661: [D loss: 0.687095, acc: 0.574219]  [A loss: 0.945094, acc: 0.132812]\n",
      "1662: [D loss: 0.679562, acc: 0.562500]  [A loss: 0.817959, acc: 0.312500]\n",
      "1663: [D loss: 0.741037, acc: 0.511719]  [A loss: 0.982682, acc: 0.078125]\n",
      "1664: [D loss: 0.724204, acc: 0.496094]  [A loss: 0.822918, acc: 0.203125]\n",
      "1665: [D loss: 0.704012, acc: 0.500000]  [A loss: 1.025643, acc: 0.101562]\n",
      "1666: [D loss: 0.701380, acc: 0.539062]  [A loss: 0.814754, acc: 0.265625]\n",
      "1667: [D loss: 0.747215, acc: 0.507812]  [A loss: 1.049851, acc: 0.054688]\n",
      "1668: [D loss: 0.681141, acc: 0.593750]  [A loss: 0.789337, acc: 0.343750]\n",
      "1669: [D loss: 0.765845, acc: 0.496094]  [A loss: 1.054955, acc: 0.054688]\n",
      "1670: [D loss: 0.716509, acc: 0.515625]  [A loss: 0.797188, acc: 0.250000]\n",
      "1671: [D loss: 0.743628, acc: 0.519531]  [A loss: 1.093352, acc: 0.031250]\n",
      "1672: [D loss: 0.702836, acc: 0.507812]  [A loss: 0.772368, acc: 0.312500]\n",
      "1673: [D loss: 0.717294, acc: 0.519531]  [A loss: 0.915376, acc: 0.117188]\n",
      "1674: [D loss: 0.695670, acc: 0.539062]  [A loss: 0.772208, acc: 0.328125]\n",
      "1675: [D loss: 0.712194, acc: 0.527344]  [A loss: 0.943952, acc: 0.093750]\n",
      "1676: [D loss: 0.697737, acc: 0.546875]  [A loss: 0.750235, acc: 0.421875]\n",
      "1677: [D loss: 0.723734, acc: 0.515625]  [A loss: 0.981403, acc: 0.101562]\n",
      "1678: [D loss: 0.695600, acc: 0.519531]  [A loss: 0.917482, acc: 0.203125]\n",
      "1679: [D loss: 0.710114, acc: 0.500000]  [A loss: 0.898737, acc: 0.179688]\n",
      "1680: [D loss: 0.688599, acc: 0.554688]  [A loss: 0.885142, acc: 0.187500]\n",
      "1681: [D loss: 0.695824, acc: 0.554688]  [A loss: 0.821774, acc: 0.281250]\n",
      "1682: [D loss: 0.705258, acc: 0.546875]  [A loss: 0.915417, acc: 0.140625]\n",
      "1683: [D loss: 0.712683, acc: 0.539062]  [A loss: 0.812615, acc: 0.273438]\n",
      "1684: [D loss: 0.703898, acc: 0.515625]  [A loss: 0.902818, acc: 0.171875]\n",
      "1685: [D loss: 0.687279, acc: 0.605469]  [A loss: 0.891243, acc: 0.210938]\n",
      "1686: [D loss: 0.698823, acc: 0.546875]  [A loss: 0.876101, acc: 0.218750]\n",
      "1687: [D loss: 0.698511, acc: 0.492188]  [A loss: 1.150312, acc: 0.031250]\n",
      "1688: [D loss: 0.688534, acc: 0.546875]  [A loss: 0.801266, acc: 0.390625]\n",
      "1689: [D loss: 0.734001, acc: 0.488281]  [A loss: 1.018642, acc: 0.078125]\n",
      "1690: [D loss: 0.711764, acc: 0.503906]  [A loss: 0.963060, acc: 0.140625]\n",
      "1691: [D loss: 0.731190, acc: 0.476562]  [A loss: 0.900731, acc: 0.164062]\n",
      "1692: [D loss: 0.728794, acc: 0.500000]  [A loss: 0.929053, acc: 0.109375]\n",
      "1693: [D loss: 0.685808, acc: 0.546875]  [A loss: 0.818370, acc: 0.242188]\n",
      "1694: [D loss: 0.696840, acc: 0.535156]  [A loss: 0.953140, acc: 0.109375]\n",
      "1695: [D loss: 0.689601, acc: 0.554688]  [A loss: 0.772335, acc: 0.359375]\n",
      "1696: [D loss: 0.751231, acc: 0.484375]  [A loss: 1.015509, acc: 0.078125]\n",
      "1697: [D loss: 0.710308, acc: 0.535156]  [A loss: 0.911845, acc: 0.148438]\n",
      "1698: [D loss: 0.706188, acc: 0.519531]  [A loss: 0.866519, acc: 0.257812]\n",
      "1699: [D loss: 0.711919, acc: 0.511719]  [A loss: 1.023234, acc: 0.125000]\n",
      "1700: [D loss: 0.720851, acc: 0.546875]  [A loss: 0.851770, acc: 0.226562]\n",
      "1701: [D loss: 0.726524, acc: 0.492188]  [A loss: 0.932577, acc: 0.140625]\n",
      "1702: [D loss: 0.698303, acc: 0.523438]  [A loss: 0.814891, acc: 0.257812]\n",
      "1703: [D loss: 0.738174, acc: 0.484375]  [A loss: 1.088444, acc: 0.007812]\n",
      "1704: [D loss: 0.696642, acc: 0.546875]  [A loss: 0.766656, acc: 0.414062]\n",
      "1705: [D loss: 0.735894, acc: 0.511719]  [A loss: 1.203558, acc: 0.000000]\n",
      "1706: [D loss: 0.681209, acc: 0.554688]  [A loss: 0.687165, acc: 0.515625]\n",
      "1707: [D loss: 0.771131, acc: 0.464844]  [A loss: 0.995137, acc: 0.062500]\n",
      "1708: [D loss: 0.683746, acc: 0.546875]  [A loss: 0.812427, acc: 0.289062]\n",
      "1709: [D loss: 0.742384, acc: 0.468750]  [A loss: 0.983187, acc: 0.062500]\n",
      "1710: [D loss: 0.693531, acc: 0.527344]  [A loss: 0.786409, acc: 0.281250]\n",
      "1711: [D loss: 0.712460, acc: 0.503906]  [A loss: 1.010970, acc: 0.039062]\n",
      "1712: [D loss: 0.684069, acc: 0.562500]  [A loss: 0.667585, acc: 0.593750]\n",
      "1713: [D loss: 0.745149, acc: 0.523438]  [A loss: 1.056113, acc: 0.046875]\n",
      "1714: [D loss: 0.726533, acc: 0.523438]  [A loss: 0.801241, acc: 0.273438]\n",
      "1715: [D loss: 0.713451, acc: 0.542969]  [A loss: 1.032467, acc: 0.015625]\n",
      "1716: [D loss: 0.690325, acc: 0.570312]  [A loss: 0.801824, acc: 0.273438]\n",
      "1717: [D loss: 0.711554, acc: 0.527344]  [A loss: 0.904976, acc: 0.140625]\n",
      "1718: [D loss: 0.671146, acc: 0.562500]  [A loss: 0.778887, acc: 0.312500]\n",
      "1719: [D loss: 0.727353, acc: 0.496094]  [A loss: 0.951913, acc: 0.117188]\n",
      "1720: [D loss: 0.690907, acc: 0.507812]  [A loss: 0.805485, acc: 0.312500]\n",
      "1721: [D loss: 0.706126, acc: 0.550781]  [A loss: 0.909693, acc: 0.132812]\n",
      "1722: [D loss: 0.686600, acc: 0.550781]  [A loss: 0.878044, acc: 0.242188]\n",
      "1723: [D loss: 0.722484, acc: 0.496094]  [A loss: 0.926648, acc: 0.132812]\n",
      "1724: [D loss: 0.696067, acc: 0.550781]  [A loss: 0.897828, acc: 0.179688]\n",
      "1725: [D loss: 0.730158, acc: 0.480469]  [A loss: 0.842748, acc: 0.304688]\n",
      "1726: [D loss: 0.707364, acc: 0.492188]  [A loss: 1.111619, acc: 0.039062]\n",
      "1727: [D loss: 0.692123, acc: 0.523438]  [A loss: 0.740986, acc: 0.406250]\n",
      "1728: [D loss: 0.753653, acc: 0.503906]  [A loss: 0.994331, acc: 0.085938]\n",
      "1729: [D loss: 0.694768, acc: 0.511719]  [A loss: 0.851325, acc: 0.187500]\n",
      "1730: [D loss: 0.705268, acc: 0.566406]  [A loss: 0.994007, acc: 0.085938]\n",
      "1731: [D loss: 0.696524, acc: 0.531250]  [A loss: 0.704629, acc: 0.484375]\n",
      "1732: [D loss: 0.774041, acc: 0.472656]  [A loss: 1.104168, acc: 0.015625]\n",
      "1733: [D loss: 0.705225, acc: 0.539062]  [A loss: 0.909397, acc: 0.210938]\n",
      "1734: [D loss: 0.714721, acc: 0.496094]  [A loss: 1.136805, acc: 0.046875]\n",
      "1735: [D loss: 0.725127, acc: 0.523438]  [A loss: 0.953346, acc: 0.171875]\n",
      "1736: [D loss: 0.691424, acc: 0.558594]  [A loss: 0.786224, acc: 0.343750]\n",
      "1737: [D loss: 0.731593, acc: 0.527344]  [A loss: 0.922561, acc: 0.109375]\n",
      "1738: [D loss: 0.685763, acc: 0.542969]  [A loss: 0.776886, acc: 0.351562]\n",
      "1739: [D loss: 0.713316, acc: 0.515625]  [A loss: 0.978346, acc: 0.117188]\n",
      "1740: [D loss: 0.703780, acc: 0.507812]  [A loss: 0.857520, acc: 0.203125]\n",
      "1741: [D loss: 0.708885, acc: 0.500000]  [A loss: 0.936456, acc: 0.093750]\n",
      "1742: [D loss: 0.680274, acc: 0.574219]  [A loss: 0.830950, acc: 0.242188]\n",
      "1743: [D loss: 0.719279, acc: 0.496094]  [A loss: 0.955065, acc: 0.101562]\n",
      "1744: [D loss: 0.706486, acc: 0.535156]  [A loss: 1.037982, acc: 0.109375]\n",
      "1745: [D loss: 0.678138, acc: 0.539062]  [A loss: 0.886836, acc: 0.171875]\n",
      "1746: [D loss: 0.729855, acc: 0.500000]  [A loss: 0.983559, acc: 0.078125]\n",
      "1747: [D loss: 0.674724, acc: 0.601562]  [A loss: 0.753544, acc: 0.406250]\n",
      "1748: [D loss: 0.755030, acc: 0.511719]  [A loss: 1.011161, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749: [D loss: 0.691272, acc: 0.562500]  [A loss: 0.800275, acc: 0.312500]\n",
      "1750: [D loss: 0.721243, acc: 0.496094]  [A loss: 1.086576, acc: 0.039062]\n",
      "1751: [D loss: 0.678593, acc: 0.597656]  [A loss: 0.751681, acc: 0.406250]\n",
      "1752: [D loss: 0.755202, acc: 0.539062]  [A loss: 1.067536, acc: 0.039062]\n",
      "1753: [D loss: 0.690077, acc: 0.539062]  [A loss: 0.758669, acc: 0.359375]\n",
      "1754: [D loss: 0.725100, acc: 0.507812]  [A loss: 1.036907, acc: 0.062500]\n",
      "1755: [D loss: 0.714034, acc: 0.535156]  [A loss: 0.823295, acc: 0.289062]\n",
      "1756: [D loss: 0.713559, acc: 0.496094]  [A loss: 0.937901, acc: 0.117188]\n",
      "1757: [D loss: 0.696967, acc: 0.554688]  [A loss: 0.822366, acc: 0.242188]\n",
      "1758: [D loss: 0.712851, acc: 0.531250]  [A loss: 0.924439, acc: 0.085938]\n",
      "1759: [D loss: 0.704863, acc: 0.507812]  [A loss: 0.783138, acc: 0.343750]\n",
      "1760: [D loss: 0.722710, acc: 0.554688]  [A loss: 0.956832, acc: 0.085938]\n",
      "1761: [D loss: 0.700251, acc: 0.527344]  [A loss: 0.743890, acc: 0.421875]\n",
      "1762: [D loss: 0.731578, acc: 0.539062]  [A loss: 0.927120, acc: 0.109375]\n",
      "1763: [D loss: 0.686507, acc: 0.539062]  [A loss: 0.846051, acc: 0.281250]\n",
      "1764: [D loss: 0.711436, acc: 0.562500]  [A loss: 1.073894, acc: 0.054688]\n",
      "1765: [D loss: 0.692423, acc: 0.519531]  [A loss: 0.761464, acc: 0.343750]\n",
      "1766: [D loss: 0.717697, acc: 0.515625]  [A loss: 0.936817, acc: 0.179688]\n",
      "1767: [D loss: 0.732895, acc: 0.503906]  [A loss: 0.881316, acc: 0.187500]\n",
      "1768: [D loss: 0.719700, acc: 0.507812]  [A loss: 0.843651, acc: 0.210938]\n",
      "1769: [D loss: 0.696369, acc: 0.523438]  [A loss: 0.844846, acc: 0.242188]\n",
      "1770: [D loss: 0.705885, acc: 0.511719]  [A loss: 0.925123, acc: 0.140625]\n",
      "1771: [D loss: 0.707332, acc: 0.535156]  [A loss: 0.947949, acc: 0.148438]\n",
      "1772: [D loss: 0.703433, acc: 0.511719]  [A loss: 0.822750, acc: 0.296875]\n",
      "1773: [D loss: 0.712761, acc: 0.542969]  [A loss: 0.957107, acc: 0.093750]\n",
      "1774: [D loss: 0.701299, acc: 0.527344]  [A loss: 0.799139, acc: 0.281250]\n",
      "1775: [D loss: 0.716000, acc: 0.527344]  [A loss: 0.938393, acc: 0.085938]\n",
      "1776: [D loss: 0.708214, acc: 0.515625]  [A loss: 0.799478, acc: 0.289062]\n",
      "1777: [D loss: 0.712172, acc: 0.539062]  [A loss: 0.964683, acc: 0.054688]\n",
      "1778: [D loss: 0.694333, acc: 0.546875]  [A loss: 0.835587, acc: 0.250000]\n",
      "1779: [D loss: 0.726211, acc: 0.488281]  [A loss: 1.027499, acc: 0.070312]\n",
      "1780: [D loss: 0.713156, acc: 0.515625]  [A loss: 0.744421, acc: 0.359375]\n",
      "1781: [D loss: 0.767245, acc: 0.500000]  [A loss: 1.019577, acc: 0.054688]\n",
      "1782: [D loss: 0.688661, acc: 0.542969]  [A loss: 0.610823, acc: 0.695312]\n",
      "1783: [D loss: 0.820796, acc: 0.503906]  [A loss: 1.163941, acc: 0.023438]\n",
      "1784: [D loss: 0.729983, acc: 0.488281]  [A loss: 0.683613, acc: 0.562500]\n",
      "1785: [D loss: 0.879346, acc: 0.496094]  [A loss: 1.043071, acc: 0.132812]\n",
      "1786: [D loss: 0.708009, acc: 0.515625]  [A loss: 0.854524, acc: 0.210938]\n",
      "1787: [D loss: 0.749663, acc: 0.480469]  [A loss: 1.059178, acc: 0.046875]\n",
      "1788: [D loss: 0.708691, acc: 0.503906]  [A loss: 0.770644, acc: 0.375000]\n",
      "1789: [D loss: 0.705883, acc: 0.503906]  [A loss: 0.923774, acc: 0.093750]\n",
      "1790: [D loss: 0.715614, acc: 0.484375]  [A loss: 0.832643, acc: 0.210938]\n",
      "1791: [D loss: 0.704915, acc: 0.511719]  [A loss: 0.832695, acc: 0.195312]\n",
      "1792: [D loss: 0.687516, acc: 0.578125]  [A loss: 0.876235, acc: 0.195312]\n",
      "1793: [D loss: 0.684362, acc: 0.554688]  [A loss: 0.809780, acc: 0.296875]\n",
      "1794: [D loss: 0.706366, acc: 0.519531]  [A loss: 0.914409, acc: 0.140625]\n",
      "1795: [D loss: 0.705335, acc: 0.500000]  [A loss: 0.849703, acc: 0.171875]\n",
      "1796: [D loss: 0.693781, acc: 0.566406]  [A loss: 0.849709, acc: 0.210938]\n",
      "1797: [D loss: 0.682374, acc: 0.539062]  [A loss: 0.861668, acc: 0.187500]\n",
      "1798: [D loss: 0.724666, acc: 0.511719]  [A loss: 0.900110, acc: 0.164062]\n",
      "1799: [D loss: 0.715126, acc: 0.507812]  [A loss: 0.870367, acc: 0.226562]\n",
      "1800: [D loss: 0.703457, acc: 0.503906]  [A loss: 0.871264, acc: 0.156250]\n",
      "1801: [D loss: 0.709851, acc: 0.511719]  [A loss: 0.866709, acc: 0.148438]\n",
      "1802: [D loss: 0.704479, acc: 0.542969]  [A loss: 0.906514, acc: 0.117188]\n",
      "1803: [D loss: 0.699530, acc: 0.550781]  [A loss: 0.862112, acc: 0.203125]\n",
      "1804: [D loss: 0.694466, acc: 0.542969]  [A loss: 0.862902, acc: 0.218750]\n",
      "1805: [D loss: 0.697914, acc: 0.531250]  [A loss: 0.883566, acc: 0.164062]\n",
      "1806: [D loss: 0.691921, acc: 0.535156]  [A loss: 0.949839, acc: 0.132812]\n",
      "1807: [D loss: 0.701442, acc: 0.488281]  [A loss: 0.742012, acc: 0.375000]\n",
      "1808: [D loss: 0.716021, acc: 0.500000]  [A loss: 1.050006, acc: 0.070312]\n",
      "1809: [D loss: 0.695943, acc: 0.550781]  [A loss: 0.792140, acc: 0.304688]\n",
      "1810: [D loss: 0.721599, acc: 0.492188]  [A loss: 1.012053, acc: 0.148438]\n",
      "1811: [D loss: 0.700175, acc: 0.519531]  [A loss: 0.805433, acc: 0.265625]\n",
      "1812: [D loss: 0.734730, acc: 0.496094]  [A loss: 0.987950, acc: 0.164062]\n",
      "1813: [D loss: 0.705865, acc: 0.531250]  [A loss: 0.894653, acc: 0.210938]\n",
      "1814: [D loss: 0.727964, acc: 0.500000]  [A loss: 0.850879, acc: 0.242188]\n",
      "1815: [D loss: 0.713903, acc: 0.519531]  [A loss: 0.974808, acc: 0.078125]\n",
      "1816: [D loss: 0.696373, acc: 0.519531]  [A loss: 0.732953, acc: 0.429688]\n",
      "1817: [D loss: 0.720829, acc: 0.511719]  [A loss: 1.074946, acc: 0.078125]\n",
      "1818: [D loss: 0.741004, acc: 0.480469]  [A loss: 0.857906, acc: 0.257812]\n",
      "1819: [D loss: 0.697858, acc: 0.566406]  [A loss: 0.887879, acc: 0.210938]\n",
      "1820: [D loss: 0.713457, acc: 0.531250]  [A loss: 0.933136, acc: 0.148438]\n",
      "1821: [D loss: 0.749117, acc: 0.492188]  [A loss: 1.029175, acc: 0.132812]\n",
      "1822: [D loss: 0.709432, acc: 0.515625]  [A loss: 0.912426, acc: 0.101562]\n",
      "1823: [D loss: 0.679163, acc: 0.578125]  [A loss: 0.886990, acc: 0.179688]\n",
      "1824: [D loss: 0.705456, acc: 0.527344]  [A loss: 0.860013, acc: 0.210938]\n",
      "1825: [D loss: 0.679977, acc: 0.558594]  [A loss: 0.950665, acc: 0.125000]\n",
      "1826: [D loss: 0.693803, acc: 0.574219]  [A loss: 0.911791, acc: 0.187500]\n",
      "1827: [D loss: 0.697273, acc: 0.531250]  [A loss: 0.873552, acc: 0.203125]\n",
      "1828: [D loss: 0.706270, acc: 0.507812]  [A loss: 1.040555, acc: 0.109375]\n",
      "1829: [D loss: 0.680175, acc: 0.519531]  [A loss: 0.881688, acc: 0.218750]\n",
      "1830: [D loss: 0.728447, acc: 0.523438]  [A loss: 1.348605, acc: 0.000000]\n",
      "1831: [D loss: 0.712012, acc: 0.527344]  [A loss: 0.665945, acc: 0.601562]\n",
      "1832: [D loss: 0.834768, acc: 0.492188]  [A loss: 1.151109, acc: 0.015625]\n",
      "1833: [D loss: 0.687717, acc: 0.562500]  [A loss: 0.725381, acc: 0.453125]\n",
      "1834: [D loss: 0.781068, acc: 0.488281]  [A loss: 1.086939, acc: 0.031250]\n",
      "1835: [D loss: 0.704944, acc: 0.500000]  [A loss: 0.754883, acc: 0.382812]\n",
      "1836: [D loss: 0.721475, acc: 0.511719]  [A loss: 0.882877, acc: 0.140625]\n",
      "1837: [D loss: 0.670133, acc: 0.582031]  [A loss: 0.785015, acc: 0.351562]\n",
      "1838: [D loss: 0.712298, acc: 0.531250]  [A loss: 0.873097, acc: 0.234375]\n",
      "1839: [D loss: 0.679076, acc: 0.558594]  [A loss: 0.792181, acc: 0.343750]\n",
      "1840: [D loss: 0.709493, acc: 0.562500]  [A loss: 0.892730, acc: 0.156250]\n",
      "1841: [D loss: 0.692999, acc: 0.562500]  [A loss: 0.720622, acc: 0.507812]\n",
      "1842: [D loss: 0.742679, acc: 0.550781]  [A loss: 0.926898, acc: 0.101562]\n",
      "1843: [D loss: 0.707619, acc: 0.550781]  [A loss: 0.743752, acc: 0.382812]\n",
      "1844: [D loss: 0.721222, acc: 0.511719]  [A loss: 0.981896, acc: 0.085938]\n",
      "1845: [D loss: 0.698137, acc: 0.546875]  [A loss: 0.780522, acc: 0.375000]\n",
      "1846: [D loss: 0.700733, acc: 0.546875]  [A loss: 0.911535, acc: 0.109375]\n",
      "1847: [D loss: 0.723881, acc: 0.496094]  [A loss: 1.071315, acc: 0.156250]\n",
      "1848: [D loss: 0.698316, acc: 0.500000]  [A loss: 0.817436, acc: 0.281250]\n",
      "1849: [D loss: 0.716227, acc: 0.511719]  [A loss: 0.897492, acc: 0.195312]\n",
      "1850: [D loss: 0.691985, acc: 0.570312]  [A loss: 0.807191, acc: 0.296875]\n",
      "1851: [D loss: 0.725511, acc: 0.488281]  [A loss: 0.791456, acc: 0.351562]\n",
      "1852: [D loss: 0.720862, acc: 0.535156]  [A loss: 0.942548, acc: 0.085938]\n",
      "1853: [D loss: 0.705287, acc: 0.507812]  [A loss: 0.755644, acc: 0.406250]\n",
      "1854: [D loss: 0.739559, acc: 0.507812]  [A loss: 0.948412, acc: 0.125000]\n",
      "1855: [D loss: 0.678005, acc: 0.582031]  [A loss: 0.776733, acc: 0.335938]\n",
      "1856: [D loss: 0.748794, acc: 0.488281]  [A loss: 1.009935, acc: 0.085938]\n",
      "1857: [D loss: 0.700279, acc: 0.496094]  [A loss: 0.816470, acc: 0.234375]\n",
      "1858: [D loss: 0.696290, acc: 0.511719]  [A loss: 0.948505, acc: 0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1859: [D loss: 0.690575, acc: 0.523438]  [A loss: 0.906655, acc: 0.195312]\n",
      "1860: [D loss: 0.745376, acc: 0.488281]  [A loss: 1.031288, acc: 0.093750]\n",
      "1861: [D loss: 0.694008, acc: 0.550781]  [A loss: 0.663348, acc: 0.593750]\n",
      "1862: [D loss: 0.770850, acc: 0.511719]  [A loss: 1.036101, acc: 0.062500]\n",
      "1863: [D loss: 0.690556, acc: 0.582031]  [A loss: 0.793695, acc: 0.257812]\n",
      "1864: [D loss: 0.801046, acc: 0.496094]  [A loss: 1.027561, acc: 0.054688]\n",
      "1865: [D loss: 0.710016, acc: 0.503906]  [A loss: 0.857189, acc: 0.171875]\n",
      "1866: [D loss: 0.713946, acc: 0.535156]  [A loss: 1.019423, acc: 0.078125]\n",
      "1867: [D loss: 0.697552, acc: 0.503906]  [A loss: 0.673315, acc: 0.546875]\n",
      "1868: [D loss: 0.783780, acc: 0.480469]  [A loss: 1.053186, acc: 0.031250]\n",
      "1869: [D loss: 0.693267, acc: 0.531250]  [A loss: 0.725717, acc: 0.453125]\n",
      "1870: [D loss: 0.745642, acc: 0.500000]  [A loss: 0.914639, acc: 0.148438]\n",
      "1871: [D loss: 0.702961, acc: 0.546875]  [A loss: 0.804973, acc: 0.320312]\n",
      "1872: [D loss: 0.726880, acc: 0.492188]  [A loss: 0.834877, acc: 0.242188]\n",
      "1873: [D loss: 0.696362, acc: 0.542969]  [A loss: 0.943389, acc: 0.070312]\n",
      "1874: [D loss: 0.685311, acc: 0.554688]  [A loss: 0.767283, acc: 0.304688]\n",
      "1875: [D loss: 0.714876, acc: 0.500000]  [A loss: 0.872109, acc: 0.187500]\n",
      "1876: [D loss: 0.689970, acc: 0.542969]  [A loss: 0.838458, acc: 0.195312]\n",
      "1877: [D loss: 0.692976, acc: 0.542969]  [A loss: 0.851686, acc: 0.195312]\n",
      "1878: [D loss: 0.715034, acc: 0.496094]  [A loss: 0.857449, acc: 0.210938]\n",
      "1879: [D loss: 0.698499, acc: 0.535156]  [A loss: 0.855109, acc: 0.226562]\n",
      "1880: [D loss: 0.695105, acc: 0.582031]  [A loss: 0.938889, acc: 0.140625]\n",
      "1881: [D loss: 0.701958, acc: 0.527344]  [A loss: 0.818640, acc: 0.281250]\n",
      "1882: [D loss: 0.720126, acc: 0.500000]  [A loss: 0.947450, acc: 0.101562]\n",
      "1883: [D loss: 0.705856, acc: 0.523438]  [A loss: 0.835564, acc: 0.265625]\n",
      "1884: [D loss: 0.724061, acc: 0.488281]  [A loss: 0.997156, acc: 0.078125]\n",
      "1885: [D loss: 0.716176, acc: 0.457031]  [A loss: 0.849982, acc: 0.242188]\n",
      "1886: [D loss: 0.710148, acc: 0.531250]  [A loss: 0.981273, acc: 0.132812]\n",
      "1887: [D loss: 0.705968, acc: 0.535156]  [A loss: 0.894072, acc: 0.187500]\n",
      "1888: [D loss: 0.715590, acc: 0.484375]  [A loss: 0.925019, acc: 0.140625]\n",
      "1889: [D loss: 0.725724, acc: 0.507812]  [A loss: 0.947879, acc: 0.085938]\n",
      "1890: [D loss: 0.698067, acc: 0.535156]  [A loss: 0.866171, acc: 0.203125]\n",
      "1891: [D loss: 0.725176, acc: 0.515625]  [A loss: 1.015707, acc: 0.078125]\n",
      "1892: [D loss: 0.686899, acc: 0.562500]  [A loss: 0.727737, acc: 0.468750]\n",
      "1893: [D loss: 0.763351, acc: 0.523438]  [A loss: 1.127833, acc: 0.070312]\n",
      "1894: [D loss: 0.717384, acc: 0.511719]  [A loss: 0.694634, acc: 0.554688]\n",
      "1895: [D loss: 0.858405, acc: 0.480469]  [A loss: 1.342963, acc: 0.039062]\n",
      "1896: [D loss: 0.697823, acc: 0.519531]  [A loss: 0.802493, acc: 0.289062]\n",
      "1897: [D loss: 0.808902, acc: 0.484375]  [A loss: 1.310394, acc: 0.007812]\n",
      "1898: [D loss: 0.734278, acc: 0.492188]  [A loss: 0.820284, acc: 0.312500]\n",
      "1899: [D loss: 0.699352, acc: 0.515625]  [A loss: 0.846165, acc: 0.257812]\n",
      "1900: [D loss: 0.707982, acc: 0.515625]  [A loss: 0.859265, acc: 0.179688]\n",
      "1901: [D loss: 0.697865, acc: 0.523438]  [A loss: 0.850650, acc: 0.218750]\n",
      "1902: [D loss: 0.703954, acc: 0.496094]  [A loss: 0.845872, acc: 0.257812]\n",
      "1903: [D loss: 0.684798, acc: 0.578125]  [A loss: 0.846634, acc: 0.242188]\n",
      "1904: [D loss: 0.711509, acc: 0.503906]  [A loss: 0.883080, acc: 0.148438]\n",
      "1905: [D loss: 0.683562, acc: 0.531250]  [A loss: 0.834746, acc: 0.250000]\n",
      "1906: [D loss: 0.688827, acc: 0.546875]  [A loss: 0.882438, acc: 0.234375]\n",
      "1907: [D loss: 0.717039, acc: 0.492188]  [A loss: 0.889130, acc: 0.125000]\n",
      "1908: [D loss: 0.720148, acc: 0.480469]  [A loss: 0.971009, acc: 0.085938]\n",
      "1909: [D loss: 0.690151, acc: 0.519531]  [A loss: 0.748269, acc: 0.398438]\n",
      "1910: [D loss: 0.686279, acc: 0.550781]  [A loss: 0.907994, acc: 0.164062]\n",
      "1911: [D loss: 0.705387, acc: 0.554688]  [A loss: 0.825702, acc: 0.226562]\n",
      "1912: [D loss: 0.702755, acc: 0.550781]  [A loss: 0.937523, acc: 0.140625]\n",
      "1913: [D loss: 0.693331, acc: 0.562500]  [A loss: 0.856271, acc: 0.156250]\n",
      "1914: [D loss: 0.683623, acc: 0.578125]  [A loss: 0.880634, acc: 0.179688]\n",
      "1915: [D loss: 0.688838, acc: 0.574219]  [A loss: 0.893441, acc: 0.164062]\n",
      "1916: [D loss: 0.682943, acc: 0.582031]  [A loss: 0.849817, acc: 0.289062]\n",
      "1917: [D loss: 0.702921, acc: 0.535156]  [A loss: 0.930146, acc: 0.195312]\n",
      "1918: [D loss: 0.683560, acc: 0.558594]  [A loss: 0.959182, acc: 0.117188]\n",
      "1919: [D loss: 0.720718, acc: 0.539062]  [A loss: 0.957915, acc: 0.093750]\n",
      "1920: [D loss: 0.690412, acc: 0.550781]  [A loss: 0.882268, acc: 0.187500]\n",
      "1921: [D loss: 0.685162, acc: 0.562500]  [A loss: 0.923837, acc: 0.164062]\n",
      "1922: [D loss: 0.707850, acc: 0.500000]  [A loss: 0.965639, acc: 0.148438]\n",
      "1923: [D loss: 0.705693, acc: 0.535156]  [A loss: 0.841663, acc: 0.281250]\n",
      "1924: [D loss: 0.676272, acc: 0.582031]  [A loss: 0.881133, acc: 0.234375]\n",
      "1925: [D loss: 0.721104, acc: 0.531250]  [A loss: 0.989330, acc: 0.070312]\n",
      "1926: [D loss: 0.696409, acc: 0.566406]  [A loss: 0.767117, acc: 0.390625]\n",
      "1927: [D loss: 0.743056, acc: 0.519531]  [A loss: 1.189572, acc: 0.031250]\n",
      "1928: [D loss: 0.739265, acc: 0.468750]  [A loss: 0.926395, acc: 0.164062]\n",
      "1929: [D loss: 0.683550, acc: 0.542969]  [A loss: 0.797201, acc: 0.328125]\n",
      "1930: [D loss: 0.720496, acc: 0.539062]  [A loss: 0.922368, acc: 0.218750]\n",
      "1931: [D loss: 0.707146, acc: 0.531250]  [A loss: 0.908903, acc: 0.179688]\n",
      "1932: [D loss: 0.732942, acc: 0.492188]  [A loss: 0.932857, acc: 0.164062]\n",
      "1933: [D loss: 0.701061, acc: 0.523438]  [A loss: 0.812211, acc: 0.367188]\n",
      "1934: [D loss: 0.715648, acc: 0.527344]  [A loss: 1.043358, acc: 0.046875]\n",
      "1935: [D loss: 0.721409, acc: 0.500000]  [A loss: 0.847013, acc: 0.226562]\n",
      "1936: [D loss: 0.721981, acc: 0.492188]  [A loss: 0.926754, acc: 0.203125]\n",
      "1937: [D loss: 0.709197, acc: 0.527344]  [A loss: 0.901982, acc: 0.140625]\n",
      "1938: [D loss: 0.700424, acc: 0.519531]  [A loss: 0.911607, acc: 0.140625]\n",
      "1939: [D loss: 0.697819, acc: 0.515625]  [A loss: 0.921225, acc: 0.187500]\n",
      "1940: [D loss: 0.689074, acc: 0.542969]  [A loss: 0.987510, acc: 0.101562]\n",
      "1941: [D loss: 0.690666, acc: 0.535156]  [A loss: 1.035949, acc: 0.085938]\n",
      "1942: [D loss: 0.686011, acc: 0.554688]  [A loss: 0.982656, acc: 0.109375]\n",
      "1943: [D loss: 0.715428, acc: 0.484375]  [A loss: 0.932094, acc: 0.156250]\n",
      "1944: [D loss: 0.674021, acc: 0.550781]  [A loss: 0.961772, acc: 0.140625]\n",
      "1945: [D loss: 0.727017, acc: 0.484375]  [A loss: 0.953361, acc: 0.140625]\n",
      "1946: [D loss: 0.688802, acc: 0.527344]  [A loss: 0.839835, acc: 0.265625]\n",
      "1947: [D loss: 0.678657, acc: 0.621094]  [A loss: 0.981865, acc: 0.140625]\n",
      "1948: [D loss: 0.682371, acc: 0.589844]  [A loss: 0.835225, acc: 0.382812]\n",
      "1949: [D loss: 0.736093, acc: 0.507812]  [A loss: 1.118825, acc: 0.039062]\n",
      "1950: [D loss: 0.691011, acc: 0.503906]  [A loss: 0.773471, acc: 0.382812]\n",
      "1951: [D loss: 0.748975, acc: 0.503906]  [A loss: 1.147626, acc: 0.023438]\n",
      "1952: [D loss: 0.707685, acc: 0.511719]  [A loss: 0.832045, acc: 0.242188]\n",
      "1953: [D loss: 0.712557, acc: 0.527344]  [A loss: 1.068644, acc: 0.062500]\n",
      "1954: [D loss: 0.705819, acc: 0.535156]  [A loss: 0.716558, acc: 0.531250]\n",
      "1955: [D loss: 0.731321, acc: 0.515625]  [A loss: 1.073713, acc: 0.085938]\n",
      "1956: [D loss: 0.721907, acc: 0.535156]  [A loss: 0.835692, acc: 0.257812]\n",
      "1957: [D loss: 0.702825, acc: 0.558594]  [A loss: 0.887796, acc: 0.195312]\n",
      "1958: [D loss: 0.726857, acc: 0.503906]  [A loss: 0.824889, acc: 0.273438]\n",
      "1959: [D loss: 0.720124, acc: 0.507812]  [A loss: 0.909632, acc: 0.203125]\n",
      "1960: [D loss: 0.703117, acc: 0.535156]  [A loss: 0.921180, acc: 0.125000]\n",
      "1961: [D loss: 0.709563, acc: 0.539062]  [A loss: 1.047114, acc: 0.062500]\n",
      "1962: [D loss: 0.686560, acc: 0.570312]  [A loss: 0.772303, acc: 0.398438]\n",
      "1963: [D loss: 0.714523, acc: 0.539062]  [A loss: 1.052775, acc: 0.054688]\n",
      "1964: [D loss: 0.677724, acc: 0.546875]  [A loss: 0.735135, acc: 0.453125]\n",
      "1965: [D loss: 0.725022, acc: 0.511719]  [A loss: 1.032778, acc: 0.085938]\n",
      "1966: [D loss: 0.694509, acc: 0.511719]  [A loss: 0.797250, acc: 0.281250]\n",
      "1967: [D loss: 0.694210, acc: 0.554688]  [A loss: 0.998925, acc: 0.164062]\n",
      "1968: [D loss: 0.694871, acc: 0.558594]  [A loss: 0.834653, acc: 0.296875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969: [D loss: 0.727608, acc: 0.511719]  [A loss: 1.081923, acc: 0.039062]\n",
      "1970: [D loss: 0.740184, acc: 0.496094]  [A loss: 0.898437, acc: 0.210938]\n",
      "1971: [D loss: 0.710195, acc: 0.511719]  [A loss: 0.978609, acc: 0.148438]\n",
      "1972: [D loss: 0.714699, acc: 0.500000]  [A loss: 0.847535, acc: 0.281250]\n",
      "1973: [D loss: 0.719185, acc: 0.511719]  [A loss: 1.027329, acc: 0.109375]\n",
      "1974: [D loss: 0.696515, acc: 0.546875]  [A loss: 0.800310, acc: 0.367188]\n",
      "1975: [D loss: 0.715234, acc: 0.496094]  [A loss: 1.002603, acc: 0.125000]\n",
      "1976: [D loss: 0.673701, acc: 0.566406]  [A loss: 0.814278, acc: 0.343750]\n",
      "1977: [D loss: 0.718219, acc: 0.527344]  [A loss: 0.953179, acc: 0.148438]\n",
      "1978: [D loss: 0.749476, acc: 0.488281]  [A loss: 1.076010, acc: 0.117188]\n",
      "1979: [D loss: 0.721968, acc: 0.515625]  [A loss: 0.733479, acc: 0.429688]\n",
      "1980: [D loss: 0.779682, acc: 0.488281]  [A loss: 1.217067, acc: 0.007812]\n",
      "1981: [D loss: 0.695386, acc: 0.535156]  [A loss: 0.739433, acc: 0.429688]\n",
      "1982: [D loss: 0.744543, acc: 0.531250]  [A loss: 1.030392, acc: 0.078125]\n",
      "1983: [D loss: 0.712613, acc: 0.496094]  [A loss: 0.747002, acc: 0.406250]\n",
      "1984: [D loss: 0.687674, acc: 0.535156]  [A loss: 0.833011, acc: 0.242188]\n",
      "1985: [D loss: 0.720628, acc: 0.488281]  [A loss: 0.888898, acc: 0.265625]\n",
      "1986: [D loss: 0.705410, acc: 0.531250]  [A loss: 0.956965, acc: 0.078125]\n",
      "1987: [D loss: 0.693427, acc: 0.527344]  [A loss: 0.844774, acc: 0.296875]\n",
      "1988: [D loss: 0.715916, acc: 0.500000]  [A loss: 0.966308, acc: 0.117188]\n",
      "1989: [D loss: 0.691349, acc: 0.566406]  [A loss: 0.858575, acc: 0.265625]\n",
      "1990: [D loss: 0.729367, acc: 0.488281]  [A loss: 0.992237, acc: 0.125000]\n",
      "1991: [D loss: 0.703433, acc: 0.511719]  [A loss: 0.743497, acc: 0.375000]\n",
      "1992: [D loss: 0.757121, acc: 0.496094]  [A loss: 1.069798, acc: 0.046875]\n",
      "1993: [D loss: 0.680070, acc: 0.550781]  [A loss: 0.800355, acc: 0.390625]\n",
      "1994: [D loss: 0.728420, acc: 0.484375]  [A loss: 1.304430, acc: 0.007812]\n",
      "1995: [D loss: 0.678021, acc: 0.539062]  [A loss: 0.667518, acc: 0.593750]\n",
      "1996: [D loss: 0.780145, acc: 0.488281]  [A loss: 1.153545, acc: 0.007812]\n",
      "1997: [D loss: 0.698298, acc: 0.539062]  [A loss: 0.756579, acc: 0.453125]\n",
      "1998: [D loss: 0.698504, acc: 0.535156]  [A loss: 0.903828, acc: 0.164062]\n",
      "1999: [D loss: 0.699344, acc: 0.503906]  [A loss: 0.898269, acc: 0.140625]\n"
     ]
    }
   ],
   "source": [
    "#noise input for generator output plotting\n",
    "noise_input = None\n",
    "if save_interval>0:\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "#the training loop    \n",
    "for i in range(start_step,train_steps):\n",
    "    #training the discriminator\n",
    "    images_train = x_train[np.random.randint(0,x_train.shape[0], size=batch_size), :, :, :] #selecting random real images whose count equals the batch size\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100]) # generating the noise vectors whose count equals batch size\n",
    "    images_fake = G.predict(noise) #generate fake images from generator\n",
    "    x = np.concatenate((images_train, images_fake)) #stack real and fake images\n",
    "    #generating the labels\n",
    "    y = np.ones([2*batch_size, 1]) \n",
    "    y[batch_size:, :] = 0\n",
    "    \n",
    "    x,y = shuffle(x,y) #shuffling the images and labels\n",
    "    D.trainable = True #make sure that the discriminator is trainanble\n",
    "    d_loss = D.train_on_batch(x, y) #train the discriminator on this batch of data\n",
    "    \n",
    "    #training the adverserial model, aka training the generator\n",
    "    y = np.ones([batch_size, 1]) #the labels, they are always one\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100]) #input noise for generator\n",
    "    D.trainable = False #freeze the discirminator\n",
    "    a_loss = AM.train_on_batch(noise, y) #train the adeveserial model, with the discrimimator frozen only the generator is updated\\\n",
    "                                         # and with all the labels equal (1), we are telling the generator to strive to generate data that will make the]\n",
    "                                         # discriminator always output (1) aka fooling it.\n",
    "    #print the losses and accuracy values \n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "    print(log_mesg)\n",
    "    #save the plots of generator outputs\n",
    "    if save_interval>0:\n",
    "        if (i+1)%save_interval==0:\n",
    "            plot_images(save2file=True, samples=noise_input.shape[0],noise=noise_input, step=(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
